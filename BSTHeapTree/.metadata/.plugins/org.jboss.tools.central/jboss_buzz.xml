<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Instant replay: Debugging C and C++ programs with rr</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/k-l5ALdtZH4/" /><category term="C" /><category term="Developer Tools" /><category term="Linux" /><category term="C/C++ debugger" /><category term="gdb" /><category term="GNU Debugger" /><category term="rr" /><author><name>William Cohen</name></author><id>https://developers.redhat.com/blog/?p=862007</id><updated>2021-05-03T07:00:45Z</updated><published>2021-05-03T07:00:45Z</published><content type="html">&lt;p&gt;The common theme in many time-travel movies is to go back in time to find out what went wrong and fix it. Developers also have that desire to go back in time and find why the code broke and fix it. But, often, that crucial step where everything went wrong happened long ago, and the information is no longer available.&lt;/p&gt; &lt;p&gt;The &lt;a target="_blank" rel="nofollow" href="https://rr-project.org/"&gt;rr project&lt;/a&gt; lets programmers examine the entire life of a &lt;a target="_blank" rel="nofollow" href="/topics/c"&gt;C or C++&lt;/a&gt; program run, and replay code execution to see what action in the past caused &amp;#8220;things to go horribly wrong.&amp;#8221; &lt;code&gt;rr&lt;/code&gt; is packaged with &lt;a target="_blank" rel="nofollow" href="https://fedoraproject.org/wiki/EPEL"&gt;Extra Packages for Enterprise Linux (EPEL)&lt;/a&gt; in &lt;a target="_blank" rel="nofollow" href="/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL), and with Fedora 31, 32, 33, and 34.&lt;/p&gt; &lt;p&gt;&lt;code&gt;rr&lt;/code&gt; records trace information about the execution of an application. This information allows you to repeatedly replay a particular recording of a failure and examine it in the &lt;a target="_blank" rel="nofollow" href="https://www.gnu.org/software/gdb/"&gt;GNU Debugger&lt;/a&gt; (GDB) to better investigate the cause. In addition to replaying the trace, &lt;code&gt;rr&lt;/code&gt; lets you run the program in reverse, in essence allowing you &amp;#8220;rewind the tape&amp;#8221; to see what happened earlier in the execution of the program.&lt;/p&gt; &lt;p&gt;The techniques that &lt;code&gt;rr&lt;/code&gt; provides for recording the reproducer for further examination can be a useful addition to traditional core dumps and backtraces, which give a snapshot of an issue at  a particular moment. The &lt;code&gt;rr&lt;/code&gt; recording can provide a way for developers to further investigate intermittent problems where only some application runs fail.&lt;/p&gt; &lt;p&gt;Let&amp;#8217;s see how to set up &lt;code&gt;rr&lt;/code&gt; and use it in an example to better illustrate its utility.&lt;/p&gt; &lt;h2&gt;Requirements and setup&lt;/h2&gt; &lt;p&gt;Because &lt;code&gt;rr&lt;/code&gt; uses a number of newer Linux kernel features and specific processor performance monitoring hardware, the environments it runs in are limited. The newer Fedora kernels have the needed support. However, to correctly track and recreate when asynchronous events happen in a thread, &lt;code&gt;rr&lt;/code&gt;uses performance monitoring hardware event counters that provide deterministic counts of when a thread is interrupted by an asynchronous event. Currently, &lt;code&gt;rr&lt;/code&gt; supports these counts only for Intel processors using the Westmere or newer microarchitectures.&lt;/p&gt; &lt;p&gt;Installing &lt;code&gt;rr&lt;/code&gt; on Fedora is a simple task, requiring a single RPM. If you are on RHEL, you will need to enable EPEL. Once you have access to the appropriate repositories, you can install &lt;code&gt;rr&lt;/code&gt; with the following command:&lt;/p&gt; &lt;pre&gt;$ sudo dnf -y install rr&lt;/pre&gt; &lt;p&gt;Due to the possibility that hardware performance monitoring counters might leak privileged information, many kernels default to limiting their monitoring capabilities. You will need to run the following command to allow &lt;code&gt;rr&lt;/code&gt; access to the performance monitoring counters:&lt;/p&gt; &lt;pre&gt;$ sudo sh -c 'echo 1 &amp;#62;/proc/sys/kernel/perf_event_paranoid'&lt;/pre&gt; &lt;p&gt;If you want to make that setting for the performance monitoring hardware permanent, you can also add the following line to &lt;code&gt;/etc/sysctl.conf&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;kernel.perf_event_paranoid = 1&lt;/pre&gt; &lt;h2&gt;A simple debugging example&lt;/h2&gt; &lt;p&gt;The following is a toy program that computes 2 times 0, 1, 2, and 3 in an array and prints the information:&lt;/p&gt; &lt;pre&gt;#include &amp;#60;stdio.h&amp;#62; #define SIZE 4 void zero(char *a, int size) { while (size&amp;#62;0) a[size--] = 0; } void initialize(char *a, int size) { zero(a, size); } void multiply(char *a, int size, int mult) { int i; for (i=0; i&amp;#60;size; i++) a[i] = i * mult; } void pr_array(char *a, int size) { int i; for (i=0; i&amp;#60;size; i++) printf("f(%d)=%d\n", i, a[i]); } int main(int argc, char **argv) { char a[SIZE]; int mult = 2; initialize(a, SIZE); multiply(a, SIZE, mult); pr_array(a, SIZE); return 0; } &lt;/pre&gt; &lt;p&gt;Compile it with the usual command, as follows, and include the debug option &lt;code&gt;-g&lt;/code&gt; to allow later debugging:&lt;/p&gt; &lt;pre&gt;$ gcc -g multiply.c -o multiply&lt;/pre&gt; &lt;p&gt;When you run &lt;code&gt;multiply&lt;/code&gt;, the results may be very surprising. All the results are zero. The main function is passing in 2 to the &lt;code&gt;multiply&lt;/code&gt; function. The loop in that function is very straightforward. How could this possibly go wrong?&lt;/p&gt; &lt;pre&gt;$ ./multiply f(0)=0 f(1)=0 f(2)=0 f(3)=0&lt;/pre&gt; &lt;h3&gt;Recording the issue&lt;/h3&gt; &lt;p&gt;You can investigate what is going wrong with &lt;code&gt;rr&lt;/code&gt;. Record a run of the &lt;code&gt;multiply&lt;/code&gt; program that demonstrates the issue with the following command. If the problem is intermittent, you could make multiple runs until the issue is seen:&lt;/p&gt; &lt;pre&gt;$ rr record ./multiply rr: Saving execution to trace directory `/home/wcohen/.local/share/rr/multiply-0`. f(0)=0 f(1)=0 f(2)=0 f(3)=0 &lt;/pre&gt; &lt;h3&gt;Replaying and investigating the issue&lt;/h3&gt; &lt;p&gt;To start debugging the issue, use the &lt;code&gt;rr replay&lt;/code&gt; command, which puts you into a GDB session:&lt;/p&gt; &lt;pre&gt;$ rr replay &lt;/pre&gt; &lt;p&gt;You are at the start of execution:&lt;/p&gt; &lt;pre&gt;(rr) where #0 0x00007f7f56b1f110 in _start () from /lib64/ld-linux-x86-64.so.2 #1 0x0000000000000001 in ?? () #2 0x00007ffe0bea6889 in ?? () #3 0x0000000000000000 in ?? () &lt;/pre&gt; &lt;p&gt;You can continue the program from the start and see that it has the same results as before:&lt;/p&gt; &lt;pre&gt;(rr) c Continuing. f(0)=0 f(1)=0 f(2)=0 f(3)=0 Program received signal SIGKILL, Killed. 0x0000000070000002 in ?? () &lt;/pre&gt; &lt;p&gt;You can set a breakpoint at &lt;code&gt;return 0&lt;/code&gt; in the &lt;code&gt;main&lt;/code&gt; function and work backward from there:&lt;/p&gt; &lt;pre&gt;(rr) break multiply.c:37 Breakpoint 1 at 0x401258: file multiply.c, line 37. (rr) c Continuing. f(0)=0 f(1)=0 f(2)=0 f(3)=0 Breakpoint 1, main (argc=1, argv=0x7ffe0bea5c58) at multiply.c:37 37 return 0; &lt;/pre&gt; &lt;p&gt;First, we check the values in the array &lt;code&gt;a&lt;/code&gt;. Maybe the &lt;code&gt;pr_array&lt;/code&gt; function printed wrong values. But that isn&amp;#8217;t the problem, because according to GDB, the values are all 0:&lt;/p&gt; &lt;pre&gt;(rr) print a[0] $5 = 0 '\000' (rr) print a[1] $6 = 0 '\000' (rr) print a[2] $7 = 0 '\000' (rr) print a[3] $8 = 0 '\000' &lt;/pre&gt; &lt;p&gt;Maybe &lt;code&gt;pr_array&lt;/code&gt; corrupted the values. Let&amp;#8217;s set a breakpoint on the entry to &lt;code&gt;pr_array&lt;/code&gt; and go backward in execution with a &lt;code&gt;reverse-continue&lt;/code&gt; command, to see what the state was before the &lt;code&gt;pr_array&lt;/code&gt; function executed. Looks like &lt;code&gt;pr_array&lt;/code&gt; is printing the correct values:&lt;/p&gt; &lt;pre&gt;(rr) break pr_array Breakpoint 2 at 0x4011cc: file multiply.c, line 25. (rr) reverse-continue Continuing. Breakpoint 2, pr_array (a=0x7ffe0bea5b58 "", size=4) at multiply.c:25 25 for (i=0; i&amp;#60;size; i++) (rr) print a[0] $9 = 0 '\000' (rr) print a[1] $10 = 0 '\000' (rr) print a[2] $11 = 0 '\000' (rr) print a[3] $12 = 0 '\000' &lt;/pre&gt; &lt;p&gt;Maybe something is going wrong in the &lt;code&gt;multiply&lt;/code&gt; function. Let&amp;#8217;s set a breakpoint on it and &lt;code&gt;reverse-continue&lt;/code&gt; to it:&lt;/p&gt; &lt;pre&gt;rr) break multiply Breakpoint 3 at 0x401184: file multiply.c, line 18. (rr) reverse-continue Continuing. Breakpoint 3, multiply (a=0x7ffe0bea5b58 "", size=4, mult=0) at multiply.c:18 18 for (i=0; i0) (rr) c Continuing. &lt;/pre&gt; &lt;h3&gt;Wait!&lt;/h3&gt; &lt;p&gt;What happened to the &lt;code&gt;mult&lt;/code&gt; argument? It should be 2. Zero times anything is zero. That explains the results. However, how did the &lt;code&gt;main&lt;/code&gt; function&amp;#8217;s local variable &lt;code&gt;mult&lt;/code&gt; end up being zero? It is initialized in &lt;code&gt;main&lt;/code&gt; and only passed to the compute function. Let&amp;#8217;s set a hardware watchpoint on &lt;code&gt;mult&lt;/code&gt; and continue the reverse execution of the program:&lt;/p&gt; &lt;pre&gt;(rr) up #1 0x0000000000401247 in main (argc=1, argv=0x7ffe0bea5c58) at multiply.c:35 35 multiply(a, SIZE, mult); (rr) watch -l mult Hardware watchpoint 4: -location mult (rr) reverse-continue Continuing. Hardware watchpoint 4: -location mult Old value = 0 New value = 2 zero (a=0x7ffe0bea5b58 "", size=3) at multiply.c:7 7 a[size--] = 0; &lt;/pre&gt; &lt;p&gt;Ah, now it&amp;#8217;s becoming clear how things went wrong. The &lt;code&gt;zero&lt;/code&gt; function wrote past the end of the &lt;code&gt;a&lt;/code&gt; array and overwrote the &lt;code&gt;mult&lt;/code&gt; variable even though it wasn&amp;#8217;t passed in. The &lt;code&gt;size--&lt;/code&gt; statement should be &lt;code&gt;--size&lt;/code&gt;. We can see the call to &lt;code&gt;zero&lt;/code&gt; hidden in the &lt;code&gt;initialize&lt;/code&gt; function:&lt;/p&gt; &lt;pre&gt;(rr) where #0 zero (a=0x7ffe0bea5b58 "", size=3) at multiply.c:7 #1 0x0000000000401173 in initialize (a=0x7ffe0bea5b58 "", size=4) at multiply.c:12 #2 0x0000000000401233 in main (argc=1, argv=0x7ffe0bea5c58) at multiply.c:34 &lt;/pre&gt; &lt;p&gt;If we want, we can use regular GDB continues and play it forward and go through those breakpoints we set earlier again:&lt;/p&gt; &lt;pre&gt;(rr) c Continuing. Hardware watchpoint 4: -location mult Old value = 2 New value = 0 zero (a=0x7ffe0bea5b58 "", size=3) at multiply.c:6 6 while (size&amp;#62;0) (rr) c Continuing. Breakpoint 3, multiply (a=0x7ffe0bea5b58 "", size=4, mult=0) at multiply.c:18 18 for (i=0; i&amp;#60;size; i++) (rr) c Continuing. Breakpoint 2, pr_array (a=0x7ffe0bea5b58 "", size=4) at multiply.c:25 25 for (i=0; i&amp;#60;size; i++) &lt;/pre&gt; &lt;h3&gt;Fixing the issue&lt;/h3&gt; &lt;p&gt;Now that we&amp;#8217;ve identified the problem, we can fix the &lt;code&gt;zero&lt;/code&gt; function in the &amp;#8220;new and improved&amp;#8221; &lt;code&gt;multiply2.c&lt;/code&gt; program, and things work as expected:&lt;/p&gt; &lt;pre&gt;$ gcc -g multiply2.c -o multiply2 $ ./multiply2 f(0)=0 f(1)=2 f(2)=4 f(3)=6 &lt;/pre&gt; &lt;h2&gt;Limitations of rr&lt;/h2&gt; &lt;p&gt;Although &lt;code&gt;rr&lt;/code&gt; is a useful addition to the tools that help developers find issues in programs, it does have limitations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;All the threads that &lt;code&gt;rr&lt;/code&gt; records run on a single core, so multithreaded applications will be slower.&lt;/li&gt; &lt;li&gt;&lt;code&gt;rr&lt;/code&gt; syscall monitoring is not complete, so some syscalls might slip through the cracks and not be recorded in the trace.&lt;/li&gt; &lt;li&gt;&lt;code&gt;rr&lt;/code&gt; uses &lt;code&gt;ptrace&lt;/code&gt; to monitor apps and will not work well with apps that also use &lt;code&gt;ptrace&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;code&gt;rr&lt;/code&gt; does not monitor processes outside the children of what it is recording, and misses any communication through shared memory to an outside process.&lt;/li&gt; &lt;li&gt;&lt;code&gt;rr&lt;/code&gt; operates only on very specific processor microarchitectures.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;The ability of &lt;code&gt;rr&lt;/code&gt; to go backward in the execution of a program to investigate the earlier events that lead to a problem is a useful addition to the developer&amp;#8217;s set of tools. The example in this article illustrates how you might use &lt;code&gt;rr&lt;/code&gt; to track down problems. The example was just a toy, but &lt;code&gt;rr&lt;/code&gt; has been used to track down problems in much much more substantial applications such as &lt;a target="_blank" rel="nofollow" href="https://blog.ret2.io/2018/06/19/pwn2own-2018-root-cause-analysis/"&gt;JavaScriptCore&lt;/a&gt;, the &lt;a target="_blank" rel="nofollow" href="https://animal0day.blogspot.com/2017/07/from-fuzzing-apache-httpd-server-to-cve.html"&gt;Apache httpd server&lt;/a&gt;, and &lt;a target="_blank" rel="nofollow" href="https://www.freshworks.com/saas/debugging-memory-corruption-in-production-rails-app-using-mozilla-rr-blog/"&gt;Ruby on Rails&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;See the following resources for additional information:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://rr-project.org/"&gt;rr-project page&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://arxiv.org/pdf/1705.05937.pdf"&gt;Engineering Record And Replay For Deployability Extended Technical Report&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Finstant-replay-debugging-c-and-c-programs-with-rr%2F&amp;#38;linkname=Instant%20replay%3A%20Debugging%20C%20and%20C%2B%2B%20programs%20with%20rr" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Finstant-replay-debugging-c-and-c-programs-with-rr%2F&amp;#38;linkname=Instant%20replay%3A%20Debugging%20C%20and%20C%2B%2B%20programs%20with%20rr" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Finstant-replay-debugging-c-and-c-programs-with-rr%2F&amp;#38;linkname=Instant%20replay%3A%20Debugging%20C%20and%20C%2B%2B%20programs%20with%20rr" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Finstant-replay-debugging-c-and-c-programs-with-rr%2F&amp;#38;linkname=Instant%20replay%3A%20Debugging%20C%20and%20C%2B%2B%20programs%20with%20rr" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Finstant-replay-debugging-c-and-c-programs-with-rr%2F&amp;#38;linkname=Instant%20replay%3A%20Debugging%20C%20and%20C%2B%2B%20programs%20with%20rr" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Finstant-replay-debugging-c-and-c-programs-with-rr%2F&amp;#38;linkname=Instant%20replay%3A%20Debugging%20C%20and%20C%2B%2B%20programs%20with%20rr" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Finstant-replay-debugging-c-and-c-programs-with-rr%2F&amp;#38;linkname=Instant%20replay%3A%20Debugging%20C%20and%20C%2B%2B%20programs%20with%20rr" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Finstant-replay-debugging-c-and-c-programs-with-rr%2F&amp;#038;title=Instant%20replay%3A%20Debugging%20C%20and%20C%2B%2B%20programs%20with%20rr" data-a2a-url="https://developers.redhat.com/blog/2021/05/03/instant-replay-debugging-c-and-c-programs-with-rr/" data-a2a-title="Instant replay: Debugging C and C++ programs with rr"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/05/03/instant-replay-debugging-c-and-c-programs-with-rr/"&gt;Instant replay: Debugging C and C++ programs with rr&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/k-l5ALdtZH4" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;The common theme in many time-travel movies is to go back in time to find out what went wrong and fix it. Developers also have that desire to go back in time and find why the code broke and fix it. But, often, that crucial step where everything went wrong happened long ago, and the [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/05/03/instant-replay-debugging-c-and-c-programs-with-rr/"&gt;Instant replay: Debugging C and C++ programs with rr&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/05/03/instant-replay-debugging-c-and-c-programs-with-rr/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">862007</post-id><dc:creator>William Cohen</dc:creator><dc:date>2021-05-03T07:00:45Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/05/03/instant-replay-debugging-c-and-c-programs-with-rr/</feedburner:origLink></entry><entry><title>New features in OpenMP 5.0 and 5.1</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/tH2Zp36b-pw/" /><category term="C" /><category term="Developer Tools" /><category term="Linux" /><category term="Performance" /><category term="c++11" /><category term="GCC 11" /><category term="OpenMP" /><author><name>Jakub Jelínek</name></author><id>https://developers.redhat.com/blog/?p=801277</id><updated>2021-05-03T07:00:22Z</updated><published>2021-05-03T07:00:22Z</published><content type="html">&lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://www.openmp.org"&gt;OpenMP&lt;/a&gt; is an API consisting of compiler directives and library routines for high-level parallelism in &lt;a target="_blank" rel="nofollow" href="/topics/c"&gt;C and C++&lt;/a&gt;, as well as &lt;a target="_blank" rel="nofollow" href="https://opensource.com/article/17/11/happy-60th-birthday-fortran"&gt;Fortran&lt;/a&gt;. &lt;a target="_blank" rel="nofollow" href="https://www.openmp.org/spec-html/5.1/openmp.html"&gt;Version 5.1&lt;/a&gt; of OpenMP was released in November 2020 and &lt;a target="_blank" rel="nofollow" href="https://www.openmp.org/spec-html/5.0/openmp.html"&gt;version 5.0&lt;/a&gt; was released in November 2018. This article discusses the new features from OpenMP 5.0 which are implemented in GCC 11, and some new OpenMP 5.1 features.&lt;/p&gt; &lt;h2&gt;OpenMP 5.0 features&lt;/h2&gt; &lt;p&gt;Let&amp;#8217;s start with features that were added in the OpenMP 5.0 standard version.&lt;/p&gt; &lt;h3&gt;Support for non-rectangular collapsed loops&lt;/h3&gt; &lt;p&gt;Before OpenMP 5.0, all OpenMP looping constructs (worksharing loops, &lt;code&gt;simd&lt;/code&gt;, &lt;code&gt;distribute&lt;/code&gt;, &lt;code&gt;taskloop&lt;/code&gt;, and combined or composite constructs based on those) were required to be rectangular. This means that all of the lower bound, upper bound, and increment expressions of all the associated loops in the loop nest were required to be invariant against the outermost loop. OpenMP 5.0 still requires all the increment expressions to be loop-invariant, but allows some cases where the lower and upper bound expressions of the inner loops can be based on a single outer-loop iterator.&lt;/p&gt; &lt;p&gt;There are restrictions to this new feature, however: An inner-loop iterator must use at most one outer-loop iterator, and the expressions need to resolve to &lt;em&gt;a * outer + b&lt;/em&gt;, where &lt;em&gt;a&lt;/em&gt; and &lt;em&gt;b&lt;/em&gt; are loop-invariant expressions. If the inner and referenced outer loops have different increments, there are further restrictions to support easy computation of the number of iterations of the collapsed loop nest before the loop. In addition, non-rectangular loops might not have &lt;code&gt;schedule&lt;/code&gt; or &lt;code&gt;dist_schedule&lt;/code&gt; clauses specified. This allows the implementation to choose any iteration distribution it prefers.&lt;/p&gt; &lt;p&gt;The following triangular loop is an example:&lt;/p&gt; &lt;pre&gt;#pragma omp for collapse(2) for (int i = 0; i &amp;#60; 100; i++) for (int j = 0; j &amp;#60; i; j++) arr[i][j] = compute (i, j);&lt;/pre&gt; &lt;p&gt;But a non-rectangular loop can also be much more complex:&lt;/p&gt; &lt;pre&gt;#pragma omp distribute parallel for simd collapse(4) for (int i = 0; i &amp;#60; 20; i++) for (int j = a; j &amp;#62;= g + i * h; j -= n) for (int k = 0; k &amp;#60; i; k++) for (int l = o * j; l &amp;#60; p; l += q) arr[i][j][k][l] = compute (i, j, k, l);&lt;/pre&gt; &lt;p&gt;The easiest implementation is by computing a rectangular hull of the loop nest and doing nothing inside of the combined loop body for iterations that wouldn&amp;#8217;t be run by the original loop. For example, for the first loop in this section, the implementation would be:&lt;/p&gt; &lt;pre&gt;#pragma omp for collapse(2) for (int i = 0; i &amp;#60; 100; i++) for (int j = 0; j &amp;#60; 100; j++) if (j &amp;#60; i) arr[i][j] = compute (i, j);&lt;/pre&gt; &lt;p&gt;Unfortunately, such an implementation can cause a significant work imbalance where some threads do no real work at all. Therefore, except for non-combined non-rectangular &lt;code&gt;simd&lt;/code&gt; constructs, GCC 11 computes an accurate number of iterations before the loop. In the case of loop nests with just one loop dependent on outer-loop iterators, it uses Faulhaber&amp;#8217;s formula, with adjustments for the fact that some values of the outer iterator might result in no iterations of the inner loop. This way, as long as the loop body performs roughly the same amount of work for each iteration, the work is distributed evenly.&lt;/p&gt; &lt;h3&gt;Conditional lastprivate&lt;/h3&gt; &lt;p&gt;In OpenMP, the &lt;code&gt;lastprivate&lt;/code&gt; clause can be used to retrieve the value of the privatized variable that was assigned in the last iteration of the loop. The &lt;code&gt;lastprivate&lt;/code&gt; clause with a conditional modifier works as a fancy reduction, which chooses the value from the thread (or team, SIMD lane, or task) that executed the maximum logical iteration number. For example:&lt;/p&gt; &lt;pre&gt;#pragma omp parallel for lastprivate(conditional:v) for (int i = 0; i &amp;#60; 1024; i++) if (cond (i)) v = compute (i); result (v);&lt;/pre&gt; &lt;p&gt;For this construct to work, the privatized variable must be modified only by storing directly to it, and shouldn&amp;#8217;t be modified through pointers or modified inside of other functions. This allows the implementation to find those stores easily and adjust a store to remember the logical iteration that stored it. This feature is implemented in GCC 10 already.&lt;/p&gt; &lt;h3&gt;Inclusive and exclusive scan support&lt;/h3&gt; &lt;p&gt;OpenMP 5.0 added support for implementing parallel prefix sums (otherwise known as cumulative sums or inclusive and exclusive scans). This support allows C++17 &lt;code&gt;std::inclusive_scan&lt;/code&gt; and &lt;code&gt;std::exclusive_scan&lt;/code&gt; to be parallelized using OpenMP. The syntax is built upon the &lt;code&gt;reduction&lt;/code&gt; clause with a special modifier and a new directive that divides the loop body into two halves. For example:&lt;/p&gt; &lt;pre&gt;#pragma omp parallel for reduction (inscan, +:r) for (int i = 0; i &amp;#60; 1024; i++) { r += a[i]; #pragma omp scan inclusive(r) b[i] = r; }&lt;/pre&gt; &lt;p&gt;The implementation can then split the loop into the two halves, creating not just one privatized variable per thread, but a full array for the entire construct. After evaluating one of the halves of user code for all iterations—which differs between inclusive and exclusive scans—efficient parallel computation of the prefix sum can be performed on the privatized array, and finally, the other half of the user code can be evaluated by all threads. The syntax allows the code to work properly even when the OpenMP pragmas are ignored. This feature is implemented in GCC 10.&lt;/p&gt; &lt;h3&gt;Declare variant support and meta-directives&lt;/h3&gt; &lt;p&gt;In OpenMP 5.0, some direct calls can be redirected to specialized alternative implementations based on the OpenMP context from which they are called. The specialization can be done based on which OpenMP constructs the call site is lexically nested in. The OpenMP implementation can then select the correct alternative based upon the implementation vendor, the CPU architecture and ISA flags for which the code is compiled, and so on. Here is an example:&lt;/p&gt; &lt;pre&gt;void foo_parallel_for (void); void foo_avx512 (void); void foo_ptx (void); #pragma omp declare variant (foo_parallel_for) \ match (construct={parallel,for},device={kind("any")}) #pragma omp declare variant (foo_avx512) \ match (device={isa(avx512bw,avx512vl,"avx512f")}) #pragma omp declare variant (foo_ptx) match (device={arch("nvptx")}) void foo (void);&lt;/pre&gt; &lt;p&gt;If &lt;code&gt;foo&lt;/code&gt; is called directly from within the lexical body of a worksharing loop that is lexically nested in a parallel construct (including the combined &lt;code&gt;parallel for&lt;/code&gt;), the call will be replaced by a call to &lt;code&gt;foo_parallel_for&lt;/code&gt;. If &lt;code&gt;foo&lt;/code&gt; is called from code compiled for the previously mentioned AVX512 ISAs, &lt;code&gt;foo_avx512&lt;/code&gt; will be called instead. And finally, if &lt;code&gt;foo&lt;/code&gt; is called from code running on NVidia PTX, the compiler will call &lt;code&gt;foo_ptx&lt;/code&gt; instead.&lt;/p&gt; &lt;p&gt;A complex scoring system, including user scores, decides which variant will be used in case multiple variants match. This construct is partially supported in GCC 10 and fully supported in GCC 11. The OpenMP 5.0 specification also allows meta-directives using similar syntax, where one of several different OpenMP directives can be used depending on the OpenMP context in which it is used.&lt;/p&gt; &lt;h3&gt;The loop construct&lt;/h3&gt; &lt;p&gt;In OpenMP 4.5, the various looping constructs prescribed to the implementation how it should divide the work. A programmer specified whether the work should be divided between teams in the league of teams, or between threads in the parallel region, or across SIMD lanes in a &lt;code&gt;simd&lt;/code&gt; construct, and so on. OpenMP 5.0 offers a new loop construct that is less prescriptive and leaves more freedom to the implementation about how to actually implement the work division. Here&amp;#8217;s an example:&lt;/p&gt; &lt;pre&gt;#pragma omp loop bind(thread) collapse(2) for (int i = 0; i &amp;#60; 1024; i++) for (int j = 0; j &amp;#60; 1024; j++) a[i][j] = work (i, j);&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;bind&lt;/code&gt; clause is required on orphaned constructs and specifies which kind of threads that encounter it will participate in the construct. If the pragma is lexically nested in an OpenMP construct that makes the binding obvious, the bind clause can be omitted. The implementation is allowed to use extra threads to execute the iterations. The loop construct is implemented in GCC 10.&lt;/p&gt; &lt;p&gt;There are restrictions on which OpenMP directives can appear in the body of the loop, and no OpenMP API calls can be used there. These restrictions were imposed so that the user program can&amp;#8217;t observe and rely on how the directive is actually implemented. Restrictions on work scheduling have been added in OpenMP 5.1, which is discussed next.&lt;/p&gt; &lt;h2&gt;OpenMP 5.1 features&lt;/h2&gt; &lt;p&gt;In OpenMP 5.1, C++ programs can specify OpenMP directives using C++11 attributes, in addition to the older use of pragmas. Two examples using attributes follow:&lt;/p&gt; &lt;pre&gt;[[omp::directive (parallel for, schedule(static))]] for (int i = 0; i &amp;#60; 1024; i++) a[i] = work (I); [[omp::sequence (directive (parallel, num_threads(16)), \ directive (for, schedule(static, 32)))]] for (int i = 0; i &amp;#60; 1024; i++) a[i] = work (i);&lt;/pre&gt; &lt;p&gt;OpenMP 5.1 added a scope directive, where all threads encountering it will execute the body of the construct. Private and reduction clauses can be applied to it. For example:&lt;/p&gt; &lt;pre&gt;#pragma omp scope private (i) reduction(+:r) { i = foo (); r += i; }&lt;/pre&gt; &lt;p&gt;Unless the &lt;code&gt;nowait&lt;/code&gt; clause is present on the directive, there is an implicit barrier at the end of the region.&lt;/p&gt; &lt;p&gt;OpenMP 5.1 has new &lt;code&gt;assume&lt;/code&gt;, &lt;code&gt;interop&lt;/code&gt;, &lt;code&gt;dispatch&lt;/code&gt;, &lt;code&gt;error&lt;/code&gt;, and &lt;code&gt;nothing&lt;/code&gt; directives. Loop transformation directives were also added. The master was deprecated and replaced by the new masked construct. There are many new API calls, including:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;omp_target_is_accessible&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;omp_get_mapped_ptr&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;omp_calloc&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;omp_aligned_alloc&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;omp_realloc&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;omp_set_num_teams&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;omp_set_teams_thread_limit&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;omp_get_max_teams&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;omp_get_teams_thread_limit&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The &lt;a target="_blank" rel="nofollow" href="https://www.openmp.org/spec-html/5.1/openmpap2.html#x349-524000B"&gt;OpenMP API features history appendix&lt;/a&gt; covers all changes, including deprecated features.&lt;/p&gt; &lt;h2&gt;Try it out&lt;/h2&gt; &lt;p&gt;The specifications for both OpenMP 5.0 and OpenMP 5.1 are available at &lt;a target="_blank" rel="nofollow" href="https://www.openmp.org/specifications/"&gt;openmp.org/specifications/&lt;/a&gt;, including both PDF and HTML layouts. The latest version of GCC (&lt;a target="_blank" rel="nofollow" href="https://gcc.gnu.org/gcc-11/changes.html"&gt;GCC 11&lt;/a&gt;) supports the features described in this article and various others (this time not just C and C++, but many features also for Fortran). But several other new features of OpenMP will be implemented only in later GCC versions.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Fnew-features-in-openmp-5-0-and-5-1%2F&amp;#38;linkname=New%20features%20in%20OpenMP%205.0%20and%205.1" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Fnew-features-in-openmp-5-0-and-5-1%2F&amp;#38;linkname=New%20features%20in%20OpenMP%205.0%20and%205.1" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Fnew-features-in-openmp-5-0-and-5-1%2F&amp;#38;linkname=New%20features%20in%20OpenMP%205.0%20and%205.1" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Fnew-features-in-openmp-5-0-and-5-1%2F&amp;#38;linkname=New%20features%20in%20OpenMP%205.0%20and%205.1" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Fnew-features-in-openmp-5-0-and-5-1%2F&amp;#38;linkname=New%20features%20in%20OpenMP%205.0%20and%205.1" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Fnew-features-in-openmp-5-0-and-5-1%2F&amp;#38;linkname=New%20features%20in%20OpenMP%205.0%20and%205.1" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Fnew-features-in-openmp-5-0-and-5-1%2F&amp;#38;linkname=New%20features%20in%20OpenMP%205.0%20and%205.1" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Fnew-features-in-openmp-5-0-and-5-1%2F&amp;#038;title=New%20features%20in%20OpenMP%205.0%20and%205.1" data-a2a-url="https://developers.redhat.com/blog/2021/05/03/new-features-in-openmp-5-0-and-5-1/" data-a2a-title="New features in OpenMP 5.0 and 5.1"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/05/03/new-features-in-openmp-5-0-and-5-1/"&gt;New features in OpenMP 5.0 and 5.1&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/tH2Zp36b-pw" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;OpenMP is an API consisting of compiler directives and library routines for high-level parallelism in C and C++, as well as Fortran. Version 5.1 of OpenMP was released in November 2020 and version 5.0 was released in November 2018. This article discusses the new features from OpenMP 5.0 which are implemented in GCC 11, and [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/05/03/new-features-in-openmp-5-0-and-5-1/"&gt;New features in OpenMP 5.0 and 5.1&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/05/03/new-features-in-openmp-5-0-and-5-1/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">801277</post-id><dc:creator>Jakub Jelínek</dc:creator><dc:date>2021-05-03T07:00:22Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/05/03/new-features-in-openmp-5-0-and-5-1/</feedburner:origLink></entry><entry><title>Red Hat Software Collections 3.7 and Red Hat Developer Toolset 10.1 beta versions now available</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/XPToxqimAyE/" /><category term="C" /><category term="Developer Tools" /><category term="Java" /><category term="Linux" /><category term="Performance" /><category term="developer toolset" /><category term="gcc" /><category term="RHEL" /><category term="software collections" /><author><name>Brian Gollaher</name></author><id>https://developers.redhat.com/blog/?p=901657</id><updated>2021-05-03T04:03:18Z</updated><published>2021-05-03T04:03:18Z</published><content type="html">&lt;p&gt;The latest versions of &lt;a target="_blank" rel="nofollow" href="/products/softwarecollections/overview"&gt;Red Hat Software Collections&lt;/a&gt; and &lt;a target="_blank" rel="nofollow" href="/products/developertoolset/overview"&gt;Red Hat Developer Toolset&lt;/a&gt; are available now in beta. Software Collections 3.7 delivers the latest stable versions of many popular open source runtime languages, web servers, and databases natively to the &lt;a target="_blank" rel="nofollow" href="/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; platform. These components are supported for up to five years, supporting a more consistent, efficient, and reliable developer experience.&lt;/p&gt; &lt;h2&gt;What&amp;#8217;s new in Red Hat Software Collections 3.7&lt;/h2&gt; &lt;p&gt;New and updated collections in the latest release of Red Hat Software Collections include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;PostgreSQL 13&lt;/strong&gt;: This version provides many new features and enhancements not found in version 12. Notable changes include performance improvements resulting from deduplication of B-tree index entries, improved performance for queries that use aggregates or partitioned tables, improved query planning when using extended statistics, parallelized vacuuming of indexes, and incremental sorting.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MariaDB 10.5&lt;/strong&gt;: Notable enhancements over the previously available version, 10.3, include a number of security updates, new features, and updates to the InnoDB storage engine. In addition, the MariaDB Galera Cluster has been upgraded to version 4.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ruby&lt;/strong&gt; 3.0: This version provides a number of bug fixes and enhancements not found in Ruby 2.7. This new major release brings speed improvements, introduces language to describe the types (RBS), and provides concurrent abstraction via Ractor. The new version now also separates keyword arguments from other arguments.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Java Mission Control 8.0.0 (update)&lt;/strong&gt;: This is an advanced set of tools for managing, monitoring, profiling, and troubleshooting &lt;a target="_blank" rel="nofollow" href="/topics/enterprise-java"&gt;Java&lt;/a&gt; applications. Updates to Java Mission Control include new graphs and viewers for stack traces, threads, and memory usage.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Also new in Software Collections 3.7 is Developer Toolset 10.1, which features GNU Compiler Collection (GCC) 10.2.1, a new update of the popular free software compiler collection. GCC is a curated collection of compilers, toolchains, debuggers, and other critical development tools. Additional updates in Developer Toolset 10.1 center on delivering new updates of &lt;a target="_blank" rel="nofollow" href="/topics/c"&gt;C/C++&lt;/a&gt; and Fortran debugging and &lt;a target="_blank" rel="nofollow" href="/blog/category/performance/"&gt;performance tools&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;All new collections in Software Collections 3.7 are also available as &lt;a target="_blank" rel="nofollow" href="https://connect.redhat.com/explore/red-hat-container-certification"&gt;Red Hat Certified Containers&lt;/a&gt; through the &lt;a target="_blank" rel="nofollow" href="https://catalog.redhat.com/software/containers/explore"&gt;Red Hat Ecosystem Catalog&lt;/a&gt;. This makes it easier to build and deploy applications using the supported components of Software Collections for Red Hat Enterprise Linux and &lt;a target="_blank" rel="nofollow" href="/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; environments.&lt;/p&gt; &lt;p&gt;Software Collections 3.7 continues Red Hat’s commitment to customer choice in underlying compute architecture, with availability across x86_64, ppc64, ppc64le, and s390x hardware.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Red Hat customers with active &lt;a href="https://developers.redhat.com/blog/2019/08/21/why-you-should-be-developing-on-red-hat-enterprise-linux/"&gt;Red Hat Enterprise Linux&lt;/a&gt; subscriptions can access Software Collections via the &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/solutions/472793"&gt;Red Hat Software Collections repository&lt;/a&gt;. For more information, please read the full &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_software_collections/3-beta/"&gt;beta release notes&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Fred-hat-software-collections-3-7-and-red-hat-developer-toolset-10-1-beta-versions-now-available%2F&amp;#38;linkname=Red%20Hat%20Software%20Collections%203.7%20and%20Red%20Hat%20Developer%20Toolset%2010.1%20beta%20versions%20now%20available" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Fred-hat-software-collections-3-7-and-red-hat-developer-toolset-10-1-beta-versions-now-available%2F&amp;#38;linkname=Red%20Hat%20Software%20Collections%203.7%20and%20Red%20Hat%20Developer%20Toolset%2010.1%20beta%20versions%20now%20available" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Fred-hat-software-collections-3-7-and-red-hat-developer-toolset-10-1-beta-versions-now-available%2F&amp;#38;linkname=Red%20Hat%20Software%20Collections%203.7%20and%20Red%20Hat%20Developer%20Toolset%2010.1%20beta%20versions%20now%20available" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Fred-hat-software-collections-3-7-and-red-hat-developer-toolset-10-1-beta-versions-now-available%2F&amp;#38;linkname=Red%20Hat%20Software%20Collections%203.7%20and%20Red%20Hat%20Developer%20Toolset%2010.1%20beta%20versions%20now%20available" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Fred-hat-software-collections-3-7-and-red-hat-developer-toolset-10-1-beta-versions-now-available%2F&amp;#38;linkname=Red%20Hat%20Software%20Collections%203.7%20and%20Red%20Hat%20Developer%20Toolset%2010.1%20beta%20versions%20now%20available" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Fred-hat-software-collections-3-7-and-red-hat-developer-toolset-10-1-beta-versions-now-available%2F&amp;#38;linkname=Red%20Hat%20Software%20Collections%203.7%20and%20Red%20Hat%20Developer%20Toolset%2010.1%20beta%20versions%20now%20available" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Fred-hat-software-collections-3-7-and-red-hat-developer-toolset-10-1-beta-versions-now-available%2F&amp;#38;linkname=Red%20Hat%20Software%20Collections%203.7%20and%20Red%20Hat%20Developer%20Toolset%2010.1%20beta%20versions%20now%20available" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F05%2F03%2Fred-hat-software-collections-3-7-and-red-hat-developer-toolset-10-1-beta-versions-now-available%2F&amp;#038;title=Red%20Hat%20Software%20Collections%203.7%20and%20Red%20Hat%20Developer%20Toolset%2010.1%20beta%20versions%20now%20available" data-a2a-url="https://developers.redhat.com/blog/2021/05/03/red-hat-software-collections-3-7-and-red-hat-developer-toolset-10-1-beta-versions-now-available/" data-a2a-title="Red Hat Software Collections 3.7 and Red Hat Developer Toolset 10.1 beta versions now available"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/05/03/red-hat-software-collections-3-7-and-red-hat-developer-toolset-10-1-beta-versions-now-available/"&gt;Red Hat Software Collections 3.7 and Red Hat Developer Toolset 10.1 beta versions now available&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/XPToxqimAyE" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;The latest versions of Red Hat Software Collections and Red Hat Developer Toolset are available now in beta. Software Collections 3.7 delivers the latest stable versions of many popular open source runtime languages, web servers, and databases natively to the Red Hat Enterprise Linux platform. These components are supported for up to five years, supporting [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/05/03/red-hat-software-collections-3-7-and-red-hat-developer-toolset-10-1-beta-versions-now-available/"&gt;Red Hat Software Collections 3.7 and Red Hat Developer Toolset 10.1 beta versions now available&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/05/03/red-hat-software-collections-3-7-and-red-hat-developer-toolset-10-1-beta-versions-now-available/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">901657</post-id><dc:creator>Brian Gollaher</dc:creator><dc:date>2021-05-03T04:03:18Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/05/03/red-hat-software-collections-3-7-and-red-hat-developer-toolset-10-1-beta-versions-now-available/</feedburner:origLink></entry><entry><title type="html">Versioning of utility classes in RHPAM</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/9nQPAkGSBYI/versioning-of-utility-classes-in-rhpam.html" /><author><name>Diego Torres Fuerte</name></author><id>https://blog.kie.org/2021/05/versioning-of-utility-classes-in-rhpam.html</id><updated>2021-05-02T19:20:40Z</updated><content type="html">In this post, we will describe how to perform versioning of utility classes in RHPAM. After running your first process instances in a fresh RHPAM installation, the next time you will deploy something to the environment, is when your processes and business assets have evolved. Your new business assets will require to integrate their changes with the current client applications, as well as with the running process instances. Note that the parts of the code that can possibly evolve, can be classified as: * Utility Classes * Model classes * Rules * Process definitions. Firstly, let’s define Utility Classes. We often use utility classes in the script tasks and events, mainly for mapping process variables to service models. Thus Utility Classes define common functionality. When using Utility Classes, keep in mind the following list of best practices: * Define these methods to be static. * Avoid System.out logs. * Do not use thread operations like sleep or start thread. * Keep away from calling remote services in these utility classes. THE USE CASE: CORRECTIVE REPAIRS TO MANUFACTURING MACHINERY. Let’s consider an example, where multiple sensors in a manufacturing factory, report the state of the machine’s health, and other performance indicators. These sensors trigger Decision Management Services, that consequently will start a process for parts procurement: The image above shows the “parts procurement process”. In short, the conversation models for web services are calculated with the help of PartsStorageUtil and PurchaseOrderUtil classes. RUNNING THE UTILITY CLASS IN ITS OWN VERSION The growth of the organization requires a change in the InventoryReservationRequest. When in the new version, all requests for inventory, will go through a central, and the parts requests will be dispatched to different branches. In order to program the delivery of the parts, you are required to provide the Branch Code. In conclusion, the requirement you get from the business is: While my branches will be submitting new purchase orders to the central, any request that was previously originated in the original branch can continue to be processed in that branch. Similarly, this means for my process service that: Any existing process instance running in the version 1.0 of my procurement process instance, can continue to run in its version; likewise, any new procurement order that includes the branch code for the central, will use the new version of the utility class. The idea then for example, is that: 1. You start a process instance in version 1.0. 2. There are no parts available to fulfill the materials request. 3. The Process instance reaches the Purchase Order, and it is waiting for the “Received Materials” signal. At this point, we introduce the change for the “Request Assign parts to Repair Request”. Thus, any new requests will need to include the “Branch Code” field. As a result, the existing process instance, will continue to send the “Assign Parts to Repair Request” without “Branch Code”. UTILITY CLASS AS A DEPENDENCY The utility classes can be packaged in a jar file. Consequently, the jar file can be added as a dependency of the kjar where we want to use the utility classes. This allows changing the utility classes in a single place, and then, promote its use throughout the kjars. By looking to the following image, you can notice that, in my example, I define the parts-storage-service and the purchase-orders-service web services. Notice that, in both web services, I define a domain model that dictates how its client applications can interact with them. In the image above, you can see the relationship between the kjar (machinery-repair), the utility dependencies (jbpm-purchase-order-utils, and jbpm-parts-storage-utils), and the model jars that define the interaction with the web services. STEPS FOR THE INITIAL VERSION The initial version is created using the following steps: 1. Download and build the domain model jars from:  1.   2.   2. Download and run the web services from: 1. 2.   3. Download and build the test helper library: 4. Download and build the utility libraries: 1. 2.   5. Download and build the kjar project: 6. Deploy the kjar project to a kie-server that has persistence configuration to a relational database. 7. Start 3 process instances for the machinery-repair.parts-procurement_v1_0 process id. Given that the inventory service does not have any parts, all process instances will request a purchase order, and wait for the “Received Materials” signal. STEPS TO PRODUCE THE CHANGE The following steps create the version 2 of the kjar that connects to the web services version 2, that require the “Branch Code”: 1. Change the domain models to include the Branch Code field. (Inventory Reservation Request Model). 2. Modify the web service to use the branch code when requesting and responding to an inventory reservation. 3. Adapt the utility classes to read a process variable for the Branch Code and include it as part of the payload for the web service requests. 4. Adjust the process definition to receive the “Branch Code” as a process variable. 5. Deploy the kjar with new version (suggested as 2.0) to the kie-server. (The kie-server will have 2 containers now, the 1.0, and the 2.0). 6. Start 3 process instances for the machinery-repair.parts-procurement_v1_0 process id in the latest version. 7. Signal the previously existing process instances For each change, remember to: * Increase the component version in the pom.xml. * Refactor and run the appropriate unit tests. Note that the new process instances, and the old process instances use their own versions of the utility class. This helps to provide an example on versioning of utility classes RHPAM, by packaging the utility classes as dependencies for the RHPAM kjar. INSTALL THE MODIFIED ASSETS In the following gitHub links you can find the solution to the previous exercise, install accordingly and compare with your results: 1. Download and build the domain model jar, the web service, the utility library and the kjar project from: 1. 2.   3. 4.   2. Deploy the kjar project to the same kie-server, using the same alias used in the previous section for version 1.0.0; 3. Start 3 additional process instances for machinery-repair.parts-procurement_v2_0 process id. 4. Send the signal to the first 3 process instances, and note how they continue to interact with the services version 1.0 with no interruption. 5. Send the signal to the latest 3 process instances, and note how they continue to interact with the services version 2.0 with no interruption. CONCLUSION By setting the dependencies and separating the utility classes in their own jars, the process definitions are capable of executing their own version of the utility class in the same kie-server, thus providing an option for versioning of utility classes in RHPAM. The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/9nQPAkGSBYI" height="1" width="1" alt=""/&gt;</content><dc:creator>Diego Torres Fuerte</dc:creator><feedburner:origLink>https://blog.kie.org/2021/05/versioning-of-utility-classes-in-rhpam.html</feedburner:origLink></entry><entry><title type="html">Building Dashboards using Plain Java</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/A_K5iDugtWo/building-dashboards-using-plain-java.html" /><author><name>William Siqueira</name></author><id>https://blog.kie.org/2021/04/building-dashboards-using-plain-java.html</id><updated>2021-04-30T18:44:57Z</updated><content type="html">is a great tool to author dashboards that can consume and display data from many sources, i.e. Prometheus, JDBC, Kie Server, and others. These dashboards can be later exported to a ZIP file and then be executed on . Dashboard built with DashBuilder Recently, we added a new alternative to create those dashboards: a pure Java API. The Java API exposes our existing builders and factories for data sets and visual components, but it also brings a new set of components for pages and navigation. View the code on . View the code on . In this post, we will introduce the DashBuilder Java DSL (Domain Specific Language) API and teach you how to get started with it. DASHBUIDER DSL API The API has 5 main core components. Let’s take a look at those: DATA SETS The API for data sets has as entry point the class org.dashbuilder.dataset.def.DataSetDefFactory which allows you to create your own data set definition. You can also pure Java data set using the class DataSetFactory, for example: View the code on . It is also possible to build data sets from multiple sources, such as CSV, Prometheus, SQL, and many others. It is important to notice that the same data set can have a different representation using lookups, which can be understood as data sets transformation requests. We will explore this later. COMPONENTS Components can be used in a dashboard page to display a data set or static information. The entry point to create components is the class org.dashbuilder.dsl.factory.component.ComponentFactory. From there you can refer to components in your local file system or build displayers components. For “displayer” components notice that we need a factory to build the settings, the factory is class: org.dashbuilder.displayer.DisplayerSettingsFactory, where you can build the settings for your displayer. Bear in mind that all classes that use a displayer will require a data set to be built, otherwise, an exception will be thrown when exporting the dashboard; PAGE You can build pages composed of the components mentioned in 2. Pages have rows that can have columns and finally components. Using the class org.dashbuilder.dsl.factory.page.PageFactory allow make it easy for you to create any page component; NAVIGATION You can create the pages navigation using org.dashbuilder.dsl.factory.navigation.NavigationFactory. This class can be used to define the menu of pages that will be displayed in DashBuilder Runtime; DASHBOARD This is the class that glues everything. It can be created using org.dashbuilder.dsl.factory.dashboard.DashboardFactory. Note: All these classes have builders that can be used instead of the factory Finally, when the work is done you can export the dashboard using the class org.dashbuilder.dsl.serialization.DashboardExporter. -------------------------------------------------------------------------------- HELLO WORLD DASHBOARD Let’s now, create a Hello world using our API: REQUIREMENTS 1. Java 11+ 2. Maven 3. Docker or podman STEPS 1. Create a maven project in your favorite IDE or use the following command: View the code on . Make sure to update it with the properties, dependencies, and build the section from this pom.xml: View the code on . 2. Create the class PopulationDashboard.java in package org.kie.dashbuilder with the content as seen below; It creates a dashboard and exports it to a ZIP file. View the code on . 3. On the project root, create the following directory structure. Notice that dashboards are a directory. View the code on . Make sure Dockerfile has the content as shown below View the code on . This is how the project structure should look like: View the code on . 4. Run PopulationDashboard.java in your IDE or using the command mvn clean install exec:java. After it runs the file population.zip is generated in dashbuilder-runtime/dashboards directory. This is the Dashboard you created: To visualize the dashboard you must: 1. Build the image. Inside dashbuilder-runtime directory build the image using the following docker/podman command &gt; docker build -t dashbuilder-dev . It will download DashBuilder runtime, so it may take a while. You must run this command once. 2. Then run the image: &gt; docker run -dp 8080:8080 -v ./dashboards:/tmp/dashbuilder/models:z &gt; dashbuilder-dev Once it is running you can access localhost:8080 and login as admin/admin. Every time you run PopulationDashboard Java class, the dashboard will be automatically updated. When the work is done, you can use the generated ZIP in production by using Dashbuilder Runtime in Static mode with the provided ZIP. CONCLUSION In this post, we introduced the Java API to create dashboards for Dashbuilder. In the next post, we will introduce other data set types and data set lookup, so stay tuned! The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/A_K5iDugtWo" height="1" width="1" alt=""/&gt;</content><dc:creator>William Siqueira</dc:creator><feedburner:origLink>https://blog.kie.org/2021/04/building-dashboards-using-plain-java.html</feedburner:origLink></entry><entry><title>The GDB developer’s GNU Debugger tutorial, Part 1: Getting started with the debugger</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/qz8lVbVUQow/" /><category term="C" /><category term="Developer Tools" /><category term="Linux" /><category term="Open source" /><category term="C/C++ debugger" /><category term="debugging" /><category term="gdb" /><category term="GNU Debugger" /><author><name>Keith Seitz</name></author><id>https://developers.redhat.com/blog/?p=841207</id><updated>2021-04-30T07:00:33Z</updated><published>2021-04-30T07:00:33Z</published><content type="html">&lt;p&gt;This article is the first in a series demonstrating how to use the &lt;a target="_blank" rel="nofollow" href="https://www.gnu.org/software/gdb/"&gt;GNU Debugger (GDB)&lt;/a&gt; effectively to debug applications in &lt;a target="_blank" rel="nofollow" href="/topics/c"&gt;C and C++&lt;/a&gt;. If you have limited or no experience using GDB, this series will teach you how to debug your code more efficiently. If you are already a seasoned professional using GDB, perhaps you will discover something you haven&amp;#8217;t seen before.&lt;/p&gt; &lt;p&gt;In addition to providing developer tips and tricks for many GDB commands, future articles will also cover topics such as debugging optimized code, offline debugging (core files), and server-based sessions (&lt;em&gt;aka&lt;/em&gt; &lt;code&gt;gdbserver&lt;/code&gt;, used in container debugging).&lt;/p&gt; &lt;h2&gt;Why another GDB tutorial?&lt;/h2&gt; &lt;p&gt;The majority of GDB tutorials available on the web consist of little more than introductions to the basic &lt;code&gt;list&lt;/code&gt;, &lt;code&gt;break&lt;/code&gt;, &lt;code&gt;print&lt;/code&gt;, and &lt;code&gt;run&lt;/code&gt; commands. New GDB users just might as well read (or sing) the &lt;a target="_blank" rel="nofollow" href="https://www.gnu.org/music/gdb-song.html"&gt;official GDB Song&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;Instead of simply demonstrating a handful of useful commands, each article in this series will focus on one aspect of using GDB from the perspective of someone who develops GDB. I use GDB daily, and these tips and tricks are the ones that I (and many other advanced GDB users and developers) use to streamline our debugging sessions.&lt;/p&gt; &lt;p&gt;Because this is the first article in the series, allow me to follow the recommendation of the GDB Song and start at the very beginning: How to run GDB.&lt;/p&gt; &lt;h2&gt;Compiler options&lt;/h2&gt; &lt;p&gt;Let me get the (all-too-often-not-so) obvious out of the way: For the best debugging experience, build applications without optimization and with debugging information. That is trivial advice, but GDB&amp;#8217;s public freenode.net IRC channel (#gdb) sees these issues often enough that they warrant mentioning.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: Don&amp;#8217;t debug applications with optimization if you can avoid it. Watch for a future article on optimization.&lt;/p&gt; &lt;p&gt;Optimization can cause GDB to behave in surprising ways if you are not aware of what might be happening &amp;#8220;under the covers.&amp;#8221; I always use the C compiler option &lt;code&gt;-O0&lt;/code&gt; (that&amp;#8217;s the letter &lt;em&gt;O&lt;/em&gt; followed by the number zero) to build executables during the development cycle.&lt;/p&gt; &lt;p&gt;I also always have the toolchain emit debugging information. This is accomplished with the &lt;code&gt;-g&lt;/code&gt; option. Specifying the exact debug format is no longer necessary (or desirable); DWARF has been the default debugging information format on GNU/Linux for many years. So ignore advice to use &lt;code&gt;-ggdb&lt;/code&gt; or &lt;code&gt;-gdwarf-2&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The one specific option worth adding is &lt;code&gt;-g3&lt;/code&gt;, which tells the compiler to include debugging information about the macros (&lt;code&gt;#define FOO ...&lt;/code&gt;) used in your application. These macros may then be used in GDB just like any other symbol in your program.&lt;/p&gt; &lt;p&gt;In short, for the best debugging experience, use &lt;code&gt;-g3 -O0&lt;/code&gt; when compiling your code. Some environments (such as those using GNU autotools) set environment variables (&lt;code&gt;CFLAGS&lt;/code&gt; and &lt;code&gt;CXXFLAGS&lt;/code&gt;) that control the compiler&amp;#8217;s output. Check these flags to make sure that your invocations of the compiler enable the debugging environment you want.&lt;/p&gt; &lt;p&gt;For much more information about the impact of &lt;code&gt;-g&lt;/code&gt; and &lt;code&gt;-O&lt;/code&gt; on the debugging experience, see Alexander Oliva&amp;#8217;s treatise &lt;a target="_blank" rel="nofollow" href="https://www.fsfla.org/~lxoliva/#gOlogy"&gt;GCC gOlogy: Studying the Impact of Optimizations on Debugging&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Startup scripts&lt;/h2&gt; &lt;p&gt;Before we look at actually using GDB, something must be said about how GDB starts up and what script files it executes. Upon startup, GDB will execute the commands contained in a number of system and user script files. The location and order of execution of these files are as follows:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;code&gt;/etc/gdbinit&lt;/code&gt; (not on FSF GNU GDB): In many GNU/Linux distributions, including Fedora and &lt;a target="_blank" rel="nofollow" href="/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt;, GDB looks first for the system default initialization file and executes commands contained therein. On Red Hat-based systems, this file executes any script files (including &lt;a target="_blank" rel="nofollow" href="/blog/category/python/"&gt;Python&lt;/a&gt; scripts) installed in &lt;code&gt;/etc/gdbinit.d&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;code&gt;$HOME/.gdbinit&lt;/code&gt;: GDB will then read the user&amp;#8217;s global initialization script from the home directory, if this file exists.&lt;/li&gt; &lt;li&gt;&lt;code&gt;./.gdbinit&lt;/code&gt;: Finally, GDB will look for a startup script in the current directory. Think of this as an application-specific customization file where you can add per-project user-defined commands, pretty-printers, and other customizations.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;All of these startup files contain GDB commands to execute, but they may also include Python scripts as long as they are prefaced with the &lt;code&gt;python&lt;/code&gt; command, e.g., &lt;code&gt;python print('Hello from python!')&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;My &lt;code&gt;.gdbinit&lt;/code&gt; is actually quite simple. Its most essential lines enable command history so that GDB remembers a given number of commands that were executed from a previous session. This is analogous to the shell&amp;#8217;s history mechanism and &lt;code&gt;.bash_history&lt;/code&gt;. The entire file is:&lt;/p&gt; &lt;pre&gt;set pagination off set history save on set history expansion on&lt;/pre&gt; &lt;p&gt;The first line turns off GDB&amp;#8217;s built-in paging. The next line enables saving the history (to &lt;code&gt;~/.gdb_history&lt;/code&gt; by default), and the final line enables shell-style history expansion with the exclamation point (!) character. This option is normally disabled because the exclamation point is also a logical operator in C.&lt;/p&gt; &lt;p&gt;To prevent GDB from reading initialization files, give it the &lt;code&gt;--nx&lt;/code&gt; command-line option.&lt;/p&gt; &lt;h2&gt;Getting help in GDB&lt;/h2&gt; &lt;p&gt;There are several ways to get help using GDB, including extensive—if dry—&lt;a target="_blank" rel="nofollow" href="https://sourceware.org/gdb/documentation/"&gt;documentation&lt;/a&gt; explaining every little switch, knob, and feature.&lt;/p&gt; &lt;h3&gt;GDB community resources&lt;/h3&gt; &lt;p&gt;The community offers help to users in two places:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Via email: The &lt;a target="_blank" rel="nofollow" href="https://sourceware.org/mailman/listinfo/gdb/"&gt;GDB mailing list&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Via IRC: #gdb on &lt;a target="_blank" rel="nofollow" href="http://freenode.net"&gt;freenode.net&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;However, because this article is about &lt;em&gt;using&lt;/em&gt; GDB, the easiest way for users to get help with a command is to use GDB&amp;#8217;s built-in help system, discussed next.&lt;/p&gt; &lt;h3&gt;Accessing the help system&lt;/h3&gt; &lt;p&gt;Access GDB&amp;#8217;s built-in help system via the &lt;code&gt;help&lt;/code&gt; and &lt;code&gt;apropos&lt;/code&gt; commands. Don&amp;#8217;t know how to use the &lt;code&gt;printf&lt;/code&gt; command? Ask GDB:&lt;/p&gt; &lt;pre&gt;(gdb) help printf Formatted printing, like the C "printf" function. Usage: printf "format string", ARG1, ARG2, ARG3, ..., ARGN This supports most C printf format specifications, like %s, %d, etc. (gdb)&lt;/pre&gt; &lt;p&gt;&lt;code&gt;help&lt;/code&gt; accepts the name of any GDB command or option and outputs usage information for that command or option.&lt;/p&gt; &lt;p&gt;Like all GDB commands, the &lt;code&gt;help&lt;/code&gt; command supports tab completion. This is perhaps the most useful way to figure out what types of arguments many commands accept. For instance, entering &lt;code&gt;help show ar&lt;/code&gt; and pressing the tab key will prompt you for a completion:&lt;/p&gt; &lt;pre&gt;(gdb) help show ar architecture args arm (gdb) help show ar&lt;/pre&gt; &lt;p&gt;GDB leaves you at the command prompt ready to accept further refinement of the input. Adding &lt;code&gt;g&lt;/code&gt; to the command, followed by a tab, will complete to &lt;code&gt;help show args&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;(gdb) help show args Show argument list to give program being debugged when it is started. Follow this command with any number of args, to be passed to the program. (gdb)&lt;/pre&gt; &lt;p&gt;Don&amp;#8217;t know the exact name of the command you&amp;#8217;re looking for? Use the &lt;code&gt;apropos&lt;/code&gt; command to search the help system for specific terms. Think of it as grepping the built-in help.&lt;/p&gt; &lt;p&gt;Now that you know how and where to find help, we&amp;#8217;re ready to move on to starting GDB (finally).&lt;/p&gt; &lt;h2&gt;Starting GDB&lt;/h2&gt; &lt;p&gt;Unsurprisingly, GDB accepts a large number of command-line options to change its behavior, but the most basic way to start GDB is to pass the application&amp;#8217;s name to GDB on the command line:&lt;/p&gt; &lt;pre&gt;$ gdb myprogram &lt;span style="color: magenta;"&gt;GNU gdb (GDB) Red Hat Enterprise Linux 9.2-2.el8&lt;/span&gt; Copyright (C) 2020 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later &amp;#60;http://gnu.org/licenses/gpl.html&amp;#62; This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Type "show copying" and "show warranty" for details. This GDB was configured as "x86_64-pc-linux-gnu". Type "show configuration" for configuration details. For bug reporting instructions, please see: &amp;#60;https://www.gnu.org/software/gdb/bugs/&amp;#62;. Find the GDB manual and other documentation resources online at: &amp;#60;http://www.gnu.org/software/gdb/documentation/&amp;#62;. For help, type "help". Type "apropos word" to search for commands related to "word"... Reading symbols from /home/blog/myprogram... (gdb) &lt;/pre&gt; &lt;p&gt;GDB starts up, prints out some version information (GCC Toolset 10 shown), loads the program and its debug information, and displays copyright and help messages, ending with the command prompt, &lt;code&gt;(gdb)&lt;/code&gt;. GDB is now ready to accept input.&lt;/p&gt; &lt;h3&gt;Avoiding messages: The -q or &amp;#8211;quiet option&lt;/h3&gt; &lt;p&gt;I&amp;#8217;ve seen GDB&amp;#8217;s startup message thousands of times, so I suppress (or &amp;#8220;quiet&amp;#8221;) it with the &lt;code&gt;-q&lt;/code&gt; option:&lt;/p&gt; &lt;pre&gt;$ gdb -q myprogram Reading symbols from /home/blog/myprogram... (gdb) &lt;/pre&gt; &lt;p&gt;That&amp;#8217;s much less to read. If you are really new to GDB, you might find the full startup messaging useful or soothing, but after a while, you&amp;#8217;ll also alias &lt;code&gt;gdb&lt;/code&gt; in your shell to &lt;code&gt;gdb -q&lt;/code&gt;. If you do need the suppressed information, use the &lt;code&gt;-v&lt;/code&gt; command-line option or the &lt;code&gt;show version&lt;/code&gt; command.&lt;/p&gt; &lt;h3&gt;Passing arguments: The &amp;#8211;args option&lt;/h3&gt; &lt;p&gt;Programs often require command-line arguments. GDB offers multiple ways to pass these to your program (or &amp;#8220;inferior,&amp;#8221; in GDB parlance). The two most useful ways are to pass application arguments via the &lt;code&gt;run&lt;/code&gt; command or at startup via the &lt;code&gt;--args&lt;/code&gt; command-line option. If your application is normally started with &lt;code&gt;myprogram 1 2 3 4&lt;/code&gt;, simply preface this with &lt;code&gt;gdb -q --args&lt;/code&gt; and GDB will remember how your application should be run:&lt;/p&gt; &lt;pre&gt;$ gdb -q --args myprogram 1 2 3 4 Reading symbols from &lt;span style="color: green;"&gt;/home/blog/myprogram&lt;/span&gt;... (gdb) show args Argument list to give program being debugged when it is started is "1 2 3 4". (gdb) run Starting program: /home/blog/myprogram 1 2 3 4 [Inferior 1 (process 1596525) exited normally] $ &lt;/pre&gt; &lt;h3&gt;Attaching to a running process: The &amp;#8211;pid option&lt;/h3&gt; &lt;p&gt;If an application is already running and gets &amp;#8220;stuck,&amp;#8221; you might want to look inside to find out why. Just give GDB the process ID of your application with &lt;code&gt;--pid&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;$ sleep 100000 &amp;#38; [1] 1591979 $ gdb -q --pid 1591979 Attaching to process 1591979 Reading symbols from &lt;span style="color: green;"&gt;/usr/bin/sleep&lt;/span&gt;... Reading symbols from &lt;span style="color: green;"&gt;.gnu_debugdata for /usr/bin/sleep&lt;/span&gt;... (No debugging symbols found in &lt;span style="color: green;"&gt;.gnu_debugdata for /usr/bin/sleep&lt;/span&gt;) Reading symbols from &lt;span style="color: green;"&gt;/lib64/libc.so.6&lt;/span&gt;... Reading symbols from &lt;span style="color: green;"&gt;/usr/lib/debug/usr/lib64/libc-2.31.so.debug&lt;/span&gt;... Reading symbols from &lt;span style="color: green;"&gt;/lib64/ld-linux-x86-64.so.2&lt;/span&gt;... Reading symbols from &lt;span style="color: green;"&gt;/usr/lib/debug/usr/lib64/ld-2.31.so.debug&lt;/span&gt;... &lt;span style="color: #0000cc;"&gt;0x00007fc421d5ef98&lt;/span&gt; in &lt;span style="color: #bbbb00;"&gt;__GI___clock_nanosleep&lt;/span&gt; (&lt;span style="color: #00bbbb;"&gt;requested_time=requested_time@entry&lt;/span&gt;=0, &lt;span style="color: #00bbbb;"&gt;remaining=remaining@entry&lt;/span&gt;=0x0) at &lt;span style="color: green;"&gt;../sysdeps/unix/sysv/linux/clock_nanosleep.c&lt;/span&gt;:28 28 &lt;span style="color: #6666dd;"&gt;return&lt;/span&gt; &lt;span style="color: grey;"&gt;SYSCALL_CANCEL&lt;/span&gt; &lt;span style="color: red;"&gt;(&lt;/span&gt;nanosleep&lt;span style="color: red;"&gt;,&lt;/span&gt; requested_time&lt;span style="color: red;"&gt;,&lt;/span&gt; remaining&lt;span style="color: red;"&gt;)&lt;/span&gt; (gdb) &lt;/pre&gt; &lt;p&gt;With this option, GDB automatically loads symbols for programs that have build ID information, such as distribution-supplied packages, and interrupts the program so that you can interact with it. Look for more on how and where GDB finds debug information in a future article.&lt;/p&gt; &lt;h3&gt;Following up on a failure: The &amp;#8211;core option&lt;/h3&gt; &lt;p&gt;If your process aborted and dumped core, use the &lt;code&gt;--core&lt;/code&gt; option to tell GDB to load the core file. If the core file contains the build ID of the aborted process, GDB automatically loads that binary and its debugging information if it can. Most developers, however, need to pass an executable to GDB with this option:&lt;/p&gt; &lt;pre&gt;$ ./abort-me Aborted (core dumped) $ gdb -q abort-me --core core.2127239 Reading symbols from &lt;span style="color: green;"&gt;abort-me&lt;/span&gt;... [New LWP 2127239] Core was generated by `./abort-me'. Program terminated with signal SIGABRT, Aborted. #0 &lt;span style="color: #bbbb00;"&gt;__GI_raise&lt;/span&gt; (&lt;span style="color: #00bbbb;"&gt;sig=sig@entry&lt;/span&gt;=6) at &lt;span style="color: green;"&gt;../sysdeps/unix/sysv/linux/raise.c&lt;/span&gt;:50 50 &lt;span style="color: #6666dd;"&gt;return&lt;/span&gt; ret&lt;span style="color: red;"&gt;;&lt;/span&gt; (gdb) &lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Tip&lt;/strong&gt;: Can&amp;#8217;t find a core file? On GNU/Linux systems using systemd, check &lt;code&gt;ulimit -c&lt;/code&gt; to see whether the shell is preventing programs from creating core files. If the value is &lt;code&gt;unlimited&lt;/code&gt;, use &lt;code&gt;coredumpctl&lt;/code&gt; to find the core file. Alternatively, run &lt;code&gt;sysctl -w kernel.core_pattern=core&lt;/code&gt; to configure systemd to output core files named &lt;code&gt;core.&lt;/code&gt;&lt;em&gt;&lt;code&gt;PID&lt;/code&gt;&lt;/em&gt;, as I have for the previous example.&lt;/p&gt; &lt;h3&gt;Expedited command execution: The &amp;#8211;ex, &amp;#8211;iex, &amp;#8211;x, and &amp;#8211;batch options&lt;/h3&gt; &lt;p&gt;I often run GDB commands repeatedly from the shell to test for problems or run scripts. These command-line options help facilitate that. Most users will use (multiple) &lt;code&gt;--ex&lt;/code&gt; arguments to specify commands to run at startup to recreate a debugging session, e.g., &lt;code&gt;gdb -ex "break &lt;/code&gt;&lt;em&gt;&lt;code&gt;some_function&lt;/code&gt;&lt;/em&gt;&lt;code&gt; if &lt;/code&gt;&lt;em&gt;&lt;code&gt;arg1&lt;/code&gt;&lt;/em&gt;&lt;code&gt; == nullptr" -ex r &lt;/code&gt;&lt;em&gt;&lt;code&gt;myprogram&lt;/code&gt;&lt;/em&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;--ex &lt;/code&gt;&lt;em&gt;&lt;code&gt;CMD&lt;/code&gt;&lt;/em&gt; runs the GDB command &lt;em&gt;&lt;code&gt;CMD&lt;/code&gt;&lt;/em&gt; after the program (and debug information) is loaded. &lt;code&gt;--iex&lt;/code&gt; does the same, but executes &lt;em&gt;&lt;code&gt;CMD&lt;/code&gt; before&lt;/em&gt; the specified program is loaded.&lt;/li&gt; &lt;li&gt;&lt;code&gt;-x &lt;/code&gt;&lt;em&gt;&lt;code&gt;FILE&lt;/code&gt;&lt;/em&gt; executes GDB commands from &lt;em&gt;&lt;code&gt;FILE&lt;/code&gt;&lt;/em&gt; after the program is loaded and &lt;code&gt;--ex&lt;/code&gt; commands execute. I use this option most often if I need a lot of &lt;code&gt;--ex&lt;/code&gt; arguments to reproduce a specific debugging session.&lt;/li&gt; &lt;li&gt;&lt;code&gt;--batch&lt;/code&gt; causes GDB to exit immediately at the first command prompt; i.e., after all commands or scripts have run. Note that &lt;code&gt;--batch&lt;/code&gt; will silence even more output than &lt;code&gt;-q&lt;/code&gt; to facilitate using GDB in scripts:&lt;/li&gt; &lt;/ul&gt; &lt;pre&gt;$ # All commands complete without error $ gdb -batch -x hello.gdb myprogram Reading symbols from &lt;span style="color: green;"&gt;myprogram&lt;/span&gt;... hello $ echo $? 0 $ # Command raises an exception $ gdb -batch -ex "set foo bar" No symbol "foo" in current context. $ echo $? 1 $ # Demonstrate the order of script execution $ gdb -x hello.gdb -iex 'echo before\n' -ex 'echo after\n' simple &lt;span style="color: magenta;"&gt;GNU gdb (GDB) Red Hat Enterprise Linux 9.2-2.el8&lt;/span&gt; Copyright (C) 2020 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later &amp;#60;http://gnu.org/licenses/gpl.html&amp;#62; This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Type "show copying" and "show warranty" for details. This GDB was configured as "x86_64-redhat-linux-gnu". Type "show configuration" for configuration details. For bug reporting instructions, please see: &amp;#60;https://www.gnu.org/software/gdb/bugs/&amp;#62;. Find the GDB manual and other documentation resources online at: &amp;#60;http://www.gnu.org/software/gdb/documentation/&amp;#62;. For help, type "help". Type "apropos word" to search for commands related to "word"... before Reading symbols from &lt;span style="color: green;"&gt;simple&lt;/span&gt;... hello after (gdb) &lt;/pre&gt; &lt;h2&gt;Next up&lt;/h2&gt; &lt;p&gt;In this article, I&amp;#8217;ve shared details about how GDB starts up, reads scripts (and when it reads scripts), and several startup options commonly used by advanced GDB users.&lt;/p&gt; &lt;p&gt;In the next article, I will take a small detour to explain what debugging information is, how to inspect it, where GDB looks for it, and how to install it in distribution-supplied packages.&lt;/p&gt; &lt;p&gt;Do you have a suggestion or tip related to GDB scripts or startup, or a suggestion for a future topic about how to use GDB? Leave a comment on this article and share your idea with us.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fthe-gdb-developers-gnu-debugger-tutorial-part-1-getting-started-with-the-debugger%2F&amp;#38;linkname=The%20GDB%20developer%E2%80%99s%20GNU%20Debugger%20tutorial%2C%20Part%201%3A%20Getting%20started%20with%20the%20debugger" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fthe-gdb-developers-gnu-debugger-tutorial-part-1-getting-started-with-the-debugger%2F&amp;#38;linkname=The%20GDB%20developer%E2%80%99s%20GNU%20Debugger%20tutorial%2C%20Part%201%3A%20Getting%20started%20with%20the%20debugger" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fthe-gdb-developers-gnu-debugger-tutorial-part-1-getting-started-with-the-debugger%2F&amp;#38;linkname=The%20GDB%20developer%E2%80%99s%20GNU%20Debugger%20tutorial%2C%20Part%201%3A%20Getting%20started%20with%20the%20debugger" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fthe-gdb-developers-gnu-debugger-tutorial-part-1-getting-started-with-the-debugger%2F&amp;#38;linkname=The%20GDB%20developer%E2%80%99s%20GNU%20Debugger%20tutorial%2C%20Part%201%3A%20Getting%20started%20with%20the%20debugger" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fthe-gdb-developers-gnu-debugger-tutorial-part-1-getting-started-with-the-debugger%2F&amp;#38;linkname=The%20GDB%20developer%E2%80%99s%20GNU%20Debugger%20tutorial%2C%20Part%201%3A%20Getting%20started%20with%20the%20debugger" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fthe-gdb-developers-gnu-debugger-tutorial-part-1-getting-started-with-the-debugger%2F&amp;#38;linkname=The%20GDB%20developer%E2%80%99s%20GNU%20Debugger%20tutorial%2C%20Part%201%3A%20Getting%20started%20with%20the%20debugger" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fthe-gdb-developers-gnu-debugger-tutorial-part-1-getting-started-with-the-debugger%2F&amp;#38;linkname=The%20GDB%20developer%E2%80%99s%20GNU%20Debugger%20tutorial%2C%20Part%201%3A%20Getting%20started%20with%20the%20debugger" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fthe-gdb-developers-gnu-debugger-tutorial-part-1-getting-started-with-the-debugger%2F&amp;#038;title=The%20GDB%20developer%E2%80%99s%20GNU%20Debugger%20tutorial%2C%20Part%201%3A%20Getting%20started%20with%20the%20debugger" data-a2a-url="https://developers.redhat.com/blog/2021/04/30/the-gdb-developers-gnu-debugger-tutorial-part-1-getting-started-with-the-debugger/" data-a2a-title="The GDB developer’s GNU Debugger tutorial, Part 1: Getting started with the debugger"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/30/the-gdb-developers-gnu-debugger-tutorial-part-1-getting-started-with-the-debugger/"&gt;The GDB developer&amp;#8217;s GNU Debugger tutorial, Part 1: Getting started with the debugger&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/qz8lVbVUQow" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;This article is the first in a series demonstrating how to use the GNU Debugger (GDB) effectively to debug applications in C and C++. If you have limited or no experience using GDB, this series will teach you how to debug your code more efficiently. If you are already a seasoned professional using GDB, perhaps [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/30/the-gdb-developers-gnu-debugger-tutorial-part-1-getting-started-with-the-debugger/"&gt;The GDB developer&amp;#8217;s GNU Debugger tutorial, Part 1: Getting started with the debugger&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/04/30/the-gdb-developers-gnu-debugger-tutorial-part-1-getting-started-with-the-debugger/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">841207</post-id><dc:creator>Keith Seitz</dc:creator><dc:date>2021-04-30T07:00:33Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/04/30/the-gdb-developers-gnu-debugger-tutorial-part-1-getting-started-with-the-debugger/</feedburner:origLink></entry><entry><title>Detecting memory management bugs with GCC 11, Part 1: Understanding dynamic allocation</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/n7mImlVVF8s/" /><category term="C" /><category term="Linux" /><category term="Open source" /><category term="Security" /><category term="Attribute malloc" /><category term="debugging" /><category term="Dynamic allocation" /><category term="GCC 11" /><author><name>Martin Sebor</name></author><id>https://developers.redhat.com/blog/?p=871847</id><updated>2021-04-30T07:00:24Z</updated><published>2021-04-30T07:00:24Z</published><content type="html">&lt;p&gt;Memory management bugs are among the hardest to find in &lt;a target="_blank" rel="nofollow" href="/topics/c/"&gt;C and C++&lt;/a&gt; programs, and are a favorite target of exploits. These errors are difficult to debug because they involve three distinct sites in a program that are often far apart and obscured by the use of pointers: memory allocation, the use of the allocated memory, and the release of memory back to the system by deallocation. In this two-part article, we&amp;#8217;ll look at &lt;a target="_blank" rel="nofollow" href="https://gcc.gnu.org/"&gt;GNU Compiler Collection (GCC) 11&lt;/a&gt; enhancements that help detect the subset of these bugs that affect dynamically allocated memory. The enhancements discussed here have been made to the GCC core. Related improvements to the GCC static analyzer are covered by David Malcolm in his article &lt;a target="_blank" rel="nofollow" href="/blog/2021/01/28/static-analysis-updates-in-gcc-11"&gt;Static analysis updates in GCC 11&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Throughout this article, I include links to the code examples on &lt;a target="_blank" rel="nofollow" href="https://godbolt.org"&gt;Compiler Explorer&lt;/a&gt; for those who would like to experiment. You will find the links above the source code of each example.&lt;/p&gt; &lt;h2&gt;Overview of memory allocation policies&lt;/h2&gt; &lt;p&gt;Let&amp;#8217;s start with a quick breakdown of the major kinds of memory management bugs. C and C++ outline four broad kinds of storage allocation policies:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Automatic&lt;/strong&gt;: This policy allocates objects on the stack of a function. With the notable exception of the nonstandard, discouraged, yet still widely used &lt;code&gt;alloca()&lt;/code&gt; function, automatic objects are allocated when they are declared. They are then commonly referred to by name, except when they are passed to other functions by reference or when pointers are used to point to the elements of arrays. As the name implies, automatic objects are deallocated automatically, at the end of the block in which they are declared. Objects allocated by &lt;code&gt;alloca()&lt;/code&gt; are deallocated on function return.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dynamic&lt;/strong&gt;: This policy allocates objects on the heap by an explicit call to an allocation function. To avoid memory exhaustion, dynamically allocated objects must be deallocated by an explicit call to a corresponding deallocation function.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Static&lt;/strong&gt;: This policy allocates named objects that last for the duration of a program. They never go out of scope and so they are never deallocated during the program&amp;#8217;s execution.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Thread&lt;/strong&gt;: Like static, but limited in duration to a single thread of execution. Objects in this policy are automatically deallocated at the termination of the thread in which they are created.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Of these four policies, the one that represents the most common, but also the most insidious, class of problems is &lt;em&gt;dynamic allocation&lt;/em&gt;. This form of allocation is the subject of this two-part article. To be sure, plenty of bugs also have to do with automatic storage (think about uninitialized reads, or accessing a local variable after it has gone out of scope through a pointer obtained while it was still live), but we will talk about those another time.&lt;/p&gt; &lt;h2&gt;New command-line options in GCC 11&lt;/h2&gt; &lt;p&gt;Before diving into the details of the dynamic memory management bugs that GCC 11 can detect, let&amp;#8217;s quickly summarize the command-line options that control detection. All the options are enabled by default. Although they perform best with optimization enabled, they don&amp;#8217;t require it.&lt;/p&gt; &lt;p&gt;GCC 11 provides two new options and significantly enhances one that has been available for several releases:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;&lt;a target="_blank" rel="nofollow" href="https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmismatched-dealloc"&gt;-Wmismatched-dealloc&lt;/a&gt;&lt;/code&gt; controls warnings about mismatches between calls to general memory allocation and deallocation functions. This option is new in GCC 11.&lt;/li&gt; &lt;li&gt;&lt;code&gt;&lt;a target="_blank" rel="nofollow" href="https://gcc.gnu.org/onlinedocs/gcc/C_002b_002b-Dialect-Options.html#index-Wmismatched-new-delete"&gt;-Wmismatched-new-delete&lt;/a&gt;&lt;/code&gt; controls warnings about mismatches between calls specifically to &lt;code&gt;operator new()&lt;/code&gt; and &lt;code&gt;operator delete()&lt;/code&gt; in C++. This option is also new in GCC 11.&lt;/li&gt; &lt;li&gt;&lt;code&gt;&lt;a target="_blank" rel="nofollow" href="https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wfree-nonheap-object"&gt;-Wfree-nonheap-object&lt;/a&gt;&lt;/code&gt; controls warnings about invalid attempts to deallocate pointers that were not returned by dynamic allocation functions. This option has been enhanced in GCC 11.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Dynamic memory management functions&lt;/h2&gt; &lt;p&gt;The best known dynamic memory management functions in C are &lt;code&gt;calloc()&lt;/code&gt;, &lt;code&gt;malloc()&lt;/code&gt;, &lt;code&gt;realloc()&lt;/code&gt;, and &lt;code&gt;free()&lt;/code&gt;. But they are not the only ones. In addition to these C89 functions, C99 introduced &lt;code&gt;aligned_alloc()&lt;/code&gt;. POSIX adds a few of its own allocation functions to the mix, including &lt;code&gt;strdup()&lt;/code&gt;, &lt;code&gt;strndup()&lt;/code&gt;, and &lt;code&gt;tempnam()&lt;/code&gt;, among others. C library implementations often provide their own extensions. For instance, FreeBSD, Linux, and Solaris all define a function named &lt;a target="_blank" rel="nofollow" href="https://www.freebsd.org/cgi/man.cgi?query=reallocarray"&gt;&lt;code&gt;reallocarray()&lt;/code&gt;&lt;/a&gt; that&amp;#8217;s a hybrid between &lt;code&gt;calloc()&lt;/code&gt; and &lt;code&gt;realloc()&lt;/code&gt;. The pointer returned by all these allocation functions must be passed to &lt;code&gt;free()&lt;/code&gt; to be deallocated.&lt;/p&gt; &lt;p&gt;Besides functions that dynamically allocate raw memory, several other standard APIs allocate and deallocate other resources. For instance, &lt;code&gt;fopen()&lt;/code&gt;, &lt;code&gt;fdopen()&lt;/code&gt;, and the POSIX &lt;a target="_blank" rel="nofollow" href="https://pubs.opengroup.org/onlinepubs/9699919799/functions/open_memstream.html"&gt;&lt;code&gt;open_memstream()&lt;/code&gt;&lt;/a&gt; create and initialize &lt;code&gt;FILE&lt;/code&gt; objects that must then be disposed of by calling &lt;code&gt;fclose()&lt;/code&gt;; the &lt;code&gt;popen()&lt;/code&gt; function also creates &lt;code&gt;FILE&lt;/code&gt;s, but those must be closed by calling &lt;code&gt;pclose()&lt;/code&gt;. Similarly, the POSIX &lt;a target="_blank" rel="nofollow" href="https://pubs.opengroup.org/onlinepubs/9699919799/functions/newlocale.html"&gt;&lt;code&gt;newlocale()&lt;/code&gt;&lt;/a&gt; and &lt;a target="_blank" rel="nofollow" href="https://pubs.opengroup.org/onlinepubs/9699919799/functions/duplocale.html"&gt;&lt;code&gt;duplocale()&lt;/code&gt;&lt;/a&gt; functions create locales that must be destroyed by calling &lt;a target="_blank" rel="nofollow" href="https://pubs.opengroup.org/onlinepubs/9699919799/functions/freelocale.html"&gt;&lt;code&gt;freelocale()&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Finally, many third-party libraries and programs define their own functions either to allocate raw memory or to initialize objects of various types that reside in allocated memory. These functions usually return pointers to the objects to their clients. The simplest of these can be deallocated directly by calling &lt;code&gt;free()&lt;/code&gt;, but most APIs rely on their clients to destroy and deallocate objects by &amp;#8220;returning&amp;#8221; them to the appropriate deallocation function.&lt;/p&gt; &lt;p&gt;All these groups of APIs share a common theme: the allocation function in each group returns a pointer that&amp;#8217;s used to access the object and, importantly, that must eventually be passed to the appropriate deallocation function in the same group. The result of &lt;code&gt;malloc()&lt;/code&gt; must be passed to &lt;code&gt;free()&lt;/code&gt;, and that of &lt;code&gt;fopen()&lt;/code&gt; to &lt;code&gt;fclose()&lt;/code&gt;. Therefore, passing the result of &lt;code&gt;fopen()&lt;/code&gt; to &lt;code&gt;free()&lt;/code&gt; is a bug, as is calling &lt;code&gt;fclose()&lt;/code&gt; on a pointer returned from &lt;code&gt;malloc()&lt;/code&gt;. In addition, in C++, the result of a given form of &lt;code&gt;operator new()&lt;/code&gt;—either ordinary or array—must be deallocated by the corresponding form of &lt;code&gt;operator delete()&lt;/code&gt;, but not by calling &lt;code&gt;free()&lt;/code&gt; or &lt;code&gt;realloc()&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Matching allocations with deallocations&lt;/h2&gt; &lt;p&gt;Calling the wrong deallocation function to release a resource allocated by an allocation function from a different group usually leads to memory corruption. The call might crash right there and then, sometimes even with a helpful message, or might return to the caller and crash sometime later, in an area unrelated to the invalid call. Or the deallocation function might not crash at all but instead overwrite some data, leading to unpredictable behavior at some later point. Naturally, we would like to detect and prevent these bugs, not just before they make it into a product release, but ideally during code development before they are committed into the code base. The challenge is how to let our tools—compilers or static analyzers—know which functions must be used to deallocate each of the objects allocated by other functions.&lt;/p&gt; &lt;p&gt;For a subset of standard functions, the semantics and the associations can be and often are baked into the tools themselves. For example, GCC knows the effects of the standard C and C++ dynamic memory management functions and which ones go with which, but it doesn&amp;#8217;t have the same knowledge of &lt;code&gt;&amp;#60;stdio.h&amp;#62;&lt;/code&gt; functions such as &lt;code&gt;fopen()&lt;/code&gt; and &lt;code&gt;fclose()&lt;/code&gt;, or about implementation-defined extensions. Additionally, GCC knows nothing about user-defined functions.&lt;/p&gt; &lt;h2&gt;Attribute malloc&lt;/h2&gt; &lt;p&gt;Enter &lt;a target="_blank" rel="nofollow" href="https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#index-malloc-function-attribute"&gt;attribute &lt;code&gt;malloc&lt;/code&gt;&lt;/a&gt;, or more accurately, an enhancement to it implemented in GCC 11. In its traditional form, the attribute takes no arguments and simply lets GCC know that the function it applies to returns dynamically allocated memory like &lt;code&gt;malloc()&lt;/code&gt;. This property is used by GCC to make aliasing assumptions about the contents of the returned memory and emit more efficient code. GCC 11 extends attribute &lt;code&gt;malloc&lt;/code&gt; to take one or two arguments: the name of the deallocation function to call to release the allocated object and, optionally, the positional argument number to which the pointer must be passed. The same allocation function can be paired with any number of deallocation functions. For example, the following declarations designate &lt;code&gt;fclose()&lt;/code&gt; as the deallocator for &lt;code&gt;fopen()&lt;/code&gt;, &lt;code&gt;fdopen()&lt;/code&gt;, &lt;code&gt;fmemopen()&lt;/code&gt;, and &lt;code&gt;tmpfile()&lt;/code&gt;, and &lt;code&gt;pclose()&lt;/code&gt; as the only deallocator for &lt;code&gt;popen()&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt;int &lt;b&gt;fclose&lt;/b&gt; (FILE*); int &lt;b&gt;pclose&lt;/b&gt; (FILE*); __attribute__ ((malloc (&lt;strong&gt;fclose&lt;/strong&gt;, 1)))) FILE* &lt;b&gt;fdopen&lt;/b&gt; (int); __attribute__ ((malloc (&lt;strong&gt;fclose&lt;/strong&gt;, 1)))) FILE* &lt;b&gt;fopen&lt;/b&gt; (const char*, const char*); __attribute__ ((malloc (&lt;strong&gt;fclose&lt;/strong&gt;, 1)))) FILE* &lt;b&gt;fmemopen&lt;/b&gt; (void *, size_t, const char *); __attribute__ ((malloc (&lt;strong&gt;pclose&lt;/strong&gt;, 1)))) FILE* &lt;b&gt;popen&lt;/b&gt; (const char*, const char*); __attribute__ ((malloc (&lt;strong&gt;fclose&lt;/strong&gt;, 1)))) FILE* &lt;b&gt;tmpfile&lt;/b&gt; (void);&lt;/pre&gt; &lt;p&gt;Ideally, the declarations in &lt;code&gt;&amp;#60;stdio.h&amp;#62;&lt;/code&gt; and other C library headers would be decorated with the attribute&lt;code&gt;malloc&lt;/code&gt; as just shown. A patch for glibc on Linux was submitted but hasn&amp;#8217;t been approved yet. Until that happens, you can add the previous declarations to your own header to enable the same detection. The full patch can also be downloaded &lt;a target="_blank" rel="nofollow" href="https://sourceware.org/pipermail/libc-alpha/2021-January/121527.html"&gt;from sourceware.org&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Both GCC proper and the integrated static analyzer make use of the new attribute to issue similar warnings. The static analyzer detects a broader set of problems at the cost of increased compilation time.&lt;/p&gt; &lt;h2&gt;Detecting mismatched deallocations&lt;/h2&gt; &lt;p&gt;The new attribute &lt;code&gt;malloc&lt;/code&gt;is used by a number of warnings in GCC 11 to detect various memory management bugs. The &lt;code&gt;-Wmismatched-dealloc&lt;/code&gt; option controls warnings about deallocation calls with arguments returned from mismatched allocation functions. For example, given the declarations in the previous section, the call to &lt;code&gt;fclose()&lt;/code&gt; in the following function is diagnosed because the pointer passed to it was returned from an allocation function that&amp;#8217;s not associated with it: &lt;code&gt;popen()&lt;/code&gt;. The &lt;a target="_blank" rel="nofollow" href="https://godbolt.org/z/Wqfx93"&gt;&lt;code&gt;popen_pclose&lt;/code&gt;&lt;/a&gt; example shows how this works:&lt;/p&gt; &lt;pre&gt;void test_popen_fclose (void) { FILE *f = popen ("/bin/ls"); // use f fclose (f); } &lt;/pre&gt; &lt;p&gt;The compiler warning is:&lt;/p&gt; &lt;pre&gt;In function '&lt;b&gt;test_popen_fclose&lt;/b&gt;': &lt;span style="color: #ff00ff;"&gt;&lt;b&gt;warning&lt;/b&gt;&lt;/span&gt;: '&lt;b&gt;fclose&lt;/b&gt;' called on pointer returned from a mismatched allocation function [&lt;span style="color: #ff00ff;"&gt;&lt;b&gt;-Wmismatched-dealloc&lt;/b&gt;&lt;/span&gt;] 21 | &lt;b&gt;fclose (f);&lt;/b&gt; | &lt;b&gt;^~~~~~~~~~&lt;/b&gt; &lt;span style="color: #33cccc;"&gt;&lt;b&gt;note&lt;/b&gt;&lt;/span&gt;: returned from '&lt;b&gt;popen&lt;/b&gt;' 19 | &lt;b&gt;FILE *f = popen ("/bin/ls", "r");&lt;/b&gt; | &lt;b&gt;^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~&lt;/b&gt; &lt;/pre&gt; &lt;h2&gt;Conclusion to Part 1&lt;/h2&gt; &lt;p&gt;Look for the second half of this article, where I will describe more options for detecting dynamic allocation bugs. I will conclude with situations that can lead to false positive or false negative identifications.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fdetecting-memory-management-bugs-with-gcc-11-part-1-understanding-dynamic-allocation%2F&amp;#38;linkname=Detecting%20memory%20management%20bugs%20with%20GCC%2011%2C%20Part%201%3A%20Understanding%20dynamic%20allocation" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fdetecting-memory-management-bugs-with-gcc-11-part-1-understanding-dynamic-allocation%2F&amp;#38;linkname=Detecting%20memory%20management%20bugs%20with%20GCC%2011%2C%20Part%201%3A%20Understanding%20dynamic%20allocation" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fdetecting-memory-management-bugs-with-gcc-11-part-1-understanding-dynamic-allocation%2F&amp;#38;linkname=Detecting%20memory%20management%20bugs%20with%20GCC%2011%2C%20Part%201%3A%20Understanding%20dynamic%20allocation" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fdetecting-memory-management-bugs-with-gcc-11-part-1-understanding-dynamic-allocation%2F&amp;#38;linkname=Detecting%20memory%20management%20bugs%20with%20GCC%2011%2C%20Part%201%3A%20Understanding%20dynamic%20allocation" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fdetecting-memory-management-bugs-with-gcc-11-part-1-understanding-dynamic-allocation%2F&amp;#38;linkname=Detecting%20memory%20management%20bugs%20with%20GCC%2011%2C%20Part%201%3A%20Understanding%20dynamic%20allocation" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fdetecting-memory-management-bugs-with-gcc-11-part-1-understanding-dynamic-allocation%2F&amp;#38;linkname=Detecting%20memory%20management%20bugs%20with%20GCC%2011%2C%20Part%201%3A%20Understanding%20dynamic%20allocation" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fdetecting-memory-management-bugs-with-gcc-11-part-1-understanding-dynamic-allocation%2F&amp;#38;linkname=Detecting%20memory%20management%20bugs%20with%20GCC%2011%2C%20Part%201%3A%20Understanding%20dynamic%20allocation" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fdetecting-memory-management-bugs-with-gcc-11-part-1-understanding-dynamic-allocation%2F&amp;#038;title=Detecting%20memory%20management%20bugs%20with%20GCC%2011%2C%20Part%201%3A%20Understanding%20dynamic%20allocation" data-a2a-url="https://developers.redhat.com/blog/2021/04/30/detecting-memory-management-bugs-with-gcc-11-part-1-understanding-dynamic-allocation/" data-a2a-title="Detecting memory management bugs with GCC 11, Part 1: Understanding dynamic allocation"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/30/detecting-memory-management-bugs-with-gcc-11-part-1-understanding-dynamic-allocation/"&gt;Detecting memory management bugs with GCC 11, Part 1: Understanding dynamic allocation&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/n7mImlVVF8s" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Memory management bugs are among the hardest to find in C and C++ programs, and are a favorite target of exploits. These errors are difficult to debug because they involve three distinct sites in a program that are often far apart and obscured by the use of pointers: memory allocation, the use of the allocated [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/30/detecting-memory-management-bugs-with-gcc-11-part-1-understanding-dynamic-allocation/"&gt;Detecting memory management bugs with GCC 11, Part 1: Understanding dynamic allocation&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/04/30/detecting-memory-management-bugs-with-gcc-11-part-1-understanding-dynamic-allocation/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">871847</post-id><dc:creator>Martin Sebor</dc:creator><dc:date>2021-04-30T07:00:24Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/04/30/detecting-memory-management-bugs-with-gcc-11-part-1-understanding-dynamic-allocation/</feedburner:origLink></entry><entry><title>How Rust makes Rayon’s data parallelism magical</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/PJRkSOCtSEU/" /><category term="Developer Tools" /><category term="Performance" /><category term="Programming Languages" /><category term="Rust" /><category term="Concurrency" /><category term="data parallelism" /><category term="Rayon" /><author><name>Josh Stone</name></author><id>https://developers.redhat.com/blog/?p=849557</id><updated>2021-04-30T07:00:18Z</updated><published>2021-04-30T07:00:18Z</published><content type="html">&lt;p&gt;Rayon is a data parallelism library for the &lt;a target="_blank" rel="nofollow" href="/blog/category/rust/"&gt;Rust programming language&lt;/a&gt;. Common reactions from programmers who start to use Rayon express how it seems magical: &amp;#8220;I changed one line and my code now runs in parallel!&amp;#8221; As one of Rayon’s authors, I am of course glad to see happy users, but I want to dispel some of the magic and give credit where it’s due—to Rust itself.&lt;/p&gt; &lt;h2&gt;How Rust supports Rayon&amp;#8217;s parallelism&lt;/h2&gt; &lt;p&gt;A best-case &amp;#8220;magical&amp;#8221; scenario often looks something like this with a sequential iterator:&lt;/p&gt; &lt;pre&gt;let total = foo_vector.iter_mut()    .filter(|foo| foo.is_interesting())     .map(|foo| foo.heavy_computation())     .sum();&lt;/pre&gt; &lt;p&gt;To make this a parallel iterator with Rayon, simply change the first line to call &lt;code&gt;par_iter_mut()&lt;/code&gt;, as shown in the following example, and watch it light up all of your CPUs! It’s no problem that &lt;code&gt;foo_vector&lt;/code&gt; is a local variable on the stack, or that the computation might be mutating the values. The whole collection is automatically split among multiple threads, updates being processed independently without issues:&lt;/p&gt; &lt;pre&gt;let total = foo_vector.par_iter_mut()    .filter(|foo| foo.is_interesting())     .map(|foo| foo.heavy_computation())     .sum();&lt;/pre&gt; &lt;p&gt;As a Rust developer, it’s not enough just to go fast. Everything must still also be checked for memory and thread safety. That principle is maintained with Rayon. Suppose the sequential iterator had been written more like this:&lt;/p&gt; &lt;pre&gt;let mut total = 0; foo_vector.iter_mut()     .filter(|foo| foo.is_interesting())     .for_each(|foo| total += foo.heavy_computation());&lt;/pre&gt; &lt;p&gt;This code is fine sequentially, but if you try to make it parallel with &lt;code&gt;par_iter_mut()&lt;/code&gt;, the compiler will return an error:&lt;/p&gt; &lt;pre&gt;cannot assign to `total`, as it is a captured variable in a `Fn` closure &lt;/pre&gt; &lt;p&gt;There would be a data race if multiple threads tried to update &lt;code&gt;total&lt;/code&gt; at the same time. You could solve this by using an atomic type for &lt;code&gt;total&lt;/code&gt;, or by using Rayon’s built-in parallel &lt;code&gt;sum()&lt;/code&gt; or your own custom fold+reduce on the iterator.&lt;/p&gt; &lt;p&gt;But Rayon is a plain library, with no compiler integration whatsoever, so how can it know there was a data race? That magic is in the Rust language itself, just from Rayon expressing the right generic constraints.&lt;/p&gt; &lt;h2&gt;Ownership and borrowing in Rust&lt;/h2&gt; &lt;p&gt;Rust has strong semantics for the ownership of values of any type &lt;code&gt;T&lt;/code&gt;. The compiler’s static borrow-checker keeps track of where a value has been borrowed as a reference, either as &lt;code&gt;&amp;#38;mut T&lt;/code&gt; or &lt;code&gt;&amp;#38;T&lt;/code&gt;. When a value is owned without any borrows, the owner can do whatever it wants with the value. Borrowing the value as &lt;code&gt;&amp;#38;mut T&lt;/code&gt; is &lt;em&gt;exclusive access&lt;/em&gt;, where only the borrower can do anything with the value—but even the borrower must leave the underlying &lt;code&gt;T&lt;/code&gt; in a valid state. Borrowing the value as &lt;code&gt;&amp;#38;T&lt;/code&gt; is &lt;em&gt;immutable shared access&lt;/em&gt;, where both the borrower and the owner can only read the value. The borrow checker enforces all of this in statically determined regions of code (lifetimes), where only unborrowed owners of &lt;code&gt;T&lt;/code&gt; or exclusive borrowers of &lt;code&gt;&amp;#38;mut T&lt;/code&gt; are allowed to make any modifications. The only time &lt;code&gt;&amp;#38;T&lt;/code&gt; can be modified is with types built on &lt;code&gt;UnsafeCell&lt;/code&gt;, like atomics or &lt;code&gt;Mutex&lt;/code&gt;, that add runtime synchronization.&lt;/p&gt; &lt;p&gt;I recommend reading &lt;a target="_blank" rel="nofollow" href="https://limpet.net/mbrubeck/2019/02/07/rust-a-unique-perspective.html"&gt;Rust: A unique perspective&lt;/a&gt; for more on this topic.&lt;/p&gt; &lt;h2&gt;Thread safety traits&lt;/h2&gt; &lt;p&gt;There are two auto-traits controlling all of Rust’s thread safety: &lt;a target="_blank" rel="nofollow" href="https://doc.rust-lang.org/stable/std/marker/trait.Send.html"&gt;&lt;code&gt;Send&lt;/code&gt;&lt;/a&gt; and &lt;a target="_blank" rel="nofollow" href="https://doc.rust-lang.org/stable/std/marker/trait.Sync.html"&gt;&lt;code&gt;Sync&lt;/code&gt;&lt;/a&gt;. Being automatic means that these traits are inferred based on their composition: a struct will implement &lt;code&gt;Send&lt;/code&gt; or &lt;code&gt;Sync&lt;/code&gt; only if all of its fields do. If some field does not, such as a raw pointer, the traits do not apply to the struct unless a trait is added with an &lt;code&gt;unsafe impl&lt;/code&gt; declaration where the author asserts that thread safety will be maintained in some other way (at risk of undefined behavior).&lt;/p&gt; &lt;p&gt;&lt;code&gt;Send&lt;/code&gt; means that &lt;code&gt;T&lt;/code&gt; can move control to another thread. For owned values, this simply means the value can be moved entirely, and the original thread has no more access. But we can send references too. For a unique &lt;code&gt;&amp;#38;mut T&lt;/code&gt;, the reference can be sent if &lt;code&gt;T: Send&lt;/code&gt; is satisfied, passing the unique borrow to the other thread. For a shared &lt;code&gt;&amp;#38;T&lt;/code&gt;, the reference can be sent only based on the other trait, &lt;code&gt;T: Sync&lt;/code&gt;, which indicates that &lt;code&gt;T&lt;/code&gt; can be safely shared with another thread.&lt;/p&gt; &lt;p&gt;The Rustonomicon has a detailed chapter on &lt;a target="_blank" rel="nofollow" href="https://doc.rust-lang.org/stable/nomicon/send-and-sync.html"&gt;Send and Sync&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Function traits&lt;/h2&gt; &lt;p&gt;There are three traits related to the ways that functions can be called: &lt;a target="_blank" rel="nofollow" href="https://doc.rust-lang.org/stable/std/ops/trait.FnOnce.html"&gt;&lt;code&gt;FnOnce&lt;/code&gt;&lt;/a&gt;, &lt;a target="_blank" rel="nofollow" href="https://doc.rust-lang.org/stable/std/ops/trait.FnMut.html"&gt;&lt;code&gt;FnMut&lt;/code&gt;&lt;/a&gt;, and &lt;a target="_blank" rel="nofollow" href="https://doc.rust-lang.org/stable/std/ops/trait.Fn.html"&gt;&lt;code&gt;Fn&lt;/code&gt;&lt;/a&gt;. Plain functions automatically implement all three traits, but closures implement the traits depending on how the closures use their captured state. If a closure would move or consume any part of its state, it implements only &lt;code&gt;FnOnce&lt;/code&gt; called with &lt;code&gt;self&lt;/code&gt; by value, because it wouldn’t have state remaining to move or consume a second time. If a closure modifies its state, it implements &lt;code&gt;FnMut&lt;/code&gt; called with &lt;code&gt;&amp;#38;mut self&lt;/code&gt;, and it can also be called as &lt;code&gt;FnOnce&lt;/code&gt;. If a closure just reads its state, or has no state at all like a plain function, it implements &lt;code&gt;Fn&lt;/code&gt; called with &lt;code&gt;&amp;#38;self&lt;/code&gt;, as well as &lt;code&gt;FnMut&lt;/code&gt; and &lt;code&gt;FnOnce&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The blog post &lt;a target="_blank" rel="nofollow" href="https://rustyyato.github.io/rust/syntactic/sugar/2019/01/17/Closures-Magic-Functions.html"&gt;Closures: Magic Functions&lt;/a&gt; goes into detail about this implementation.&lt;/p&gt; &lt;h2&gt;Generic constraints in Rayon&lt;/h2&gt; &lt;p&gt;With these powerful tools in the Rust language, Rayon only has to specify its constraints. The parallel iterators and their items have to implement &lt;code&gt;Send&lt;/code&gt;, simply because they will be sent between threads. Iterator methods such as &lt;a target="_blank" rel="nofollow" href="https://docs.rs/rayon/1.5.0/rayon/iter/trait.ParallelIterator.html#method.filter"&gt;&lt;code&gt;filter&lt;/code&gt;&lt;/a&gt;, &lt;a target="_blank" rel="nofollow" href="https://docs.rs/rayon/1.5.0/rayon/iter/trait.ParallelIterator.html#method.map"&gt;&lt;code&gt;map&lt;/code&gt;&lt;/a&gt;, and &lt;code&gt;&lt;a target="_blank" rel="nofollow" href="https://docs.rs/rayon/1.5.0/rayon/iter/trait.ParallelIterator.html#method.for_each"&gt;for_each&lt;/a&gt;&lt;/code&gt; have a few more constraints on their callback function/closure &lt;code&gt;F&lt;/code&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It must implement &lt;code&gt;Send&lt;/code&gt; so you can send it to the thread pool.&lt;/li&gt; &lt;li&gt;It must implement &lt;code&gt;Sync&lt;/code&gt; so you can share &lt;code&gt;&amp;#38;F&lt;/code&gt; references to that callback across multiple threads.&lt;/li&gt; &lt;li&gt;It must implement &lt;code&gt;Fn&lt;/code&gt; so it can be called from any of those threads with a shared reference.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thus Rayon requires &lt;code&gt;F: Send + Sync + Fn&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Let&amp;#8217;s look again at the example that the compiler would reject:&lt;/p&gt; &lt;pre&gt;let mut total = 0; foo_vector.par_iter_mut()     .filter(|foo| foo.is_interesting())     .for_each(|foo| total += foo.heavy_computation());&lt;/pre&gt; &lt;p&gt;We&amp;#8217;ll take for granted that the type &lt;code&gt;Foo&lt;/code&gt; in this vector implements &lt;code&gt;Send&lt;/code&gt;, so it&amp;#8217;s also perfectly fine to send &lt;code&gt;&amp;#38;mut Foo&lt;/code&gt; references among threads as the parallel iterator items. The &lt;code&gt;for_each&lt;/code&gt; closure would capture a mutable reference to the accumulated &lt;code&gt;total&lt;/code&gt; variable. Assuming that the number has a simple type such as &lt;code&gt;i32&lt;/code&gt;, it would be acceptable to &lt;code&gt;Send&lt;/code&gt; the closure with &lt;code&gt;&amp;#38;mut i32&lt;/code&gt; to another thread. It would even be fine with &lt;code&gt;Sync&lt;/code&gt; to share references to that closure between threads. However, the mutation would make it &lt;code&gt;FnMut&lt;/code&gt;, requiring &lt;code&gt;&amp;#38;mut self&lt;/code&gt; to actually call it. The error from the compiler should now make sense:&lt;/p&gt; &lt;pre&gt;cannot assign to `total`, as it is a captured variable in a `Fn` closure &lt;/pre&gt; &lt;p&gt;Because Rayon requires &lt;code&gt;Fn&lt;/code&gt;here, that gets locked in by the compiler, and you&amp;#8217;re not allowed to do anything that would make it &lt;code&gt;FnMut&lt;/code&gt;. If you change the total to a type such as &lt;code&gt;AtomicI32&lt;/code&gt;, which does allow updates with a shared reference, the code will compile and work just fine:&lt;/p&gt; &lt;pre&gt;let mut total = AtomicI32::new(0); foo_vector.par_iter_mut()     .filter(|foo| foo.is_interesting())    .for_each(|foo| total.fetch_add(foo.heavy_computation(), Ordering::Relaxed));&lt;/pre&gt; &lt;p&gt;This is the impact of Rust’s &lt;a target="_blank" rel="nofollow" href="https://doc.rust-lang.org/book/ch16-00-concurrency.html"&gt;fearless concurrency&lt;/a&gt; (or parallelism)—not that you will never write bugs in threaded code, but that the compiler will catch the bugs before they can hurt you. Rayon can make it really easy to dip your toes into parallelism, feeling almost magical, but in truth Rayon doesn’t know anything about your code: it just specifies simple constraints and lets the Rust compiler do the hard work of proving it.&lt;/p&gt; &lt;p&gt;For more articles about Rust, please visit &lt;a href="https://developers.redhat.com/blog/category/rust/"&gt;Red Hat&amp;#8217;s topic page&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fhow-rust-makes-rayons-data-parallelism-magical%2F&amp;#38;linkname=How%20Rust%20makes%20Rayon%E2%80%99s%20data%20parallelism%20magical" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fhow-rust-makes-rayons-data-parallelism-magical%2F&amp;#38;linkname=How%20Rust%20makes%20Rayon%E2%80%99s%20data%20parallelism%20magical" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fhow-rust-makes-rayons-data-parallelism-magical%2F&amp;#38;linkname=How%20Rust%20makes%20Rayon%E2%80%99s%20data%20parallelism%20magical" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fhow-rust-makes-rayons-data-parallelism-magical%2F&amp;#38;linkname=How%20Rust%20makes%20Rayon%E2%80%99s%20data%20parallelism%20magical" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fhow-rust-makes-rayons-data-parallelism-magical%2F&amp;#38;linkname=How%20Rust%20makes%20Rayon%E2%80%99s%20data%20parallelism%20magical" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fhow-rust-makes-rayons-data-parallelism-magical%2F&amp;#38;linkname=How%20Rust%20makes%20Rayon%E2%80%99s%20data%20parallelism%20magical" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fhow-rust-makes-rayons-data-parallelism-magical%2F&amp;#38;linkname=How%20Rust%20makes%20Rayon%E2%80%99s%20data%20parallelism%20magical" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F30%2Fhow-rust-makes-rayons-data-parallelism-magical%2F&amp;#038;title=How%20Rust%20makes%20Rayon%E2%80%99s%20data%20parallelism%20magical" data-a2a-url="https://developers.redhat.com/blog/2021/04/30/how-rust-makes-rayons-data-parallelism-magical/" data-a2a-title="How Rust makes Rayon’s data parallelism magical"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/30/how-rust-makes-rayons-data-parallelism-magical/"&gt;How Rust makes Rayon&amp;#8217;s data parallelism magical&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/PJRkSOCtSEU" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Rayon is a data parallelism library for the Rust programming language. Common reactions from programmers who start to use Rayon express how it seems magical: &amp;#8220;I changed one line and my code now runs in parallel!&amp;#8221; As one of Rayon’s authors, I am of course glad to see happy users, but I want to dispel [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/30/how-rust-makes-rayons-data-parallelism-magical/"&gt;How Rust makes Rayon&amp;#8217;s data parallelism magical&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/04/30/how-rust-makes-rayons-data-parallelism-magical/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">849557</post-id><dc:creator>Josh Stone</dc:creator><dc:date>2021-04-30T07:00:18Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/04/30/how-rust-makes-rayons-data-parallelism-magical/</feedburner:origLink></entry><entry><title>Automatic load balancing for PMD threads in Open vSwitch with DPDK</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/InvJnZZDzLo/" /><category term="Kubernetes" /><category term="Linux" /><category term="Performance" /><category term="load balancing" /><category term="Open vSwitch" /><category term="OpenStack" /><category term="ovs-dpdk" /><author><name>Kevin Traynor</name></author><id>https://developers.redhat.com/blog/?p=871477</id><updated>2021-04-29T07:00:51Z</updated><published>2021-04-29T07:00:51Z</published><content type="html">&lt;p&gt;This article is about the poll mode driver (PMD) automatic load balance feature in &lt;a target="_blank" rel="nofollow" href="https://docs.openvswitch.org/en/latest/intro/install/dpdk/"&gt;Open vSwitch with a Data Plane Development Kit&lt;/a&gt; data path (OVS-DPDK). The feature has existed for a while but we&amp;#8217;ve recently added new user parameters in Open vSwitch 2.15. Now is a good time to take a look at this feature in OVS-DPDK.&lt;/p&gt; &lt;p&gt;When you are finished reading this article, you will understand the problem the PMD auto load balance feature addresses and the user parameters required to operate it. Then, you can try it out for yourself.&lt;/p&gt; &lt;h2&gt;PMD threads in Open vSwitch with DPDK&lt;/h2&gt; &lt;p&gt;In the context of OVS-DPDK a &lt;em&gt;PMD thread&lt;/em&gt;, or &lt;em&gt;poll mode driver thread&lt;/em&gt;, is a thread that runs 1:1 on a dedicated core to continually poll ports for packets. When it receives packets, it processes and usually forwards them depending on the rules that the packets match.&lt;/p&gt; &lt;p&gt;Each PMD thread is assigned a group of Rx queues from the various ports attached to the OVS-DPDK bridges to poll. Typically, the ports are DPDK physical network interface controllers (NICs) and &lt;code&gt;vhost-user&lt;/code&gt; ports.&lt;/p&gt; &lt;p&gt;You can select how many and which cores are to be used for PMD threads. Increasing the number of cores makes more processing cycles available for packet processing, which can increase OVS-DPDK throughput.&lt;/p&gt; &lt;p&gt;For example, the following command selects cores 8 and 10 to be used by PMD threads:&lt;/p&gt; &lt;pre&gt;$ ovs-vsctl set Open_vSwitch . other_config:pmd-cpu-mask=0x500&lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: We will refer a lot to the &lt;em&gt;load&lt;/em&gt; of a PMD thread; this is the number of processing cycles the PMD thread uses for receiving and processing packets on its core.&lt;/p&gt; &lt;h2&gt;PMD threads and the packet processing load&lt;/h2&gt; &lt;p&gt;All Rx queues will not carry the same traffic, and some might have no traffic at all, so some PMD threads will do more packet processing than others. In other words, the packet processing load is not balanced across PMD threads.&lt;/p&gt; &lt;p&gt;In the worst case, some PMD threads could be overloaded processing packets while other PMD threads, possibly added to increase throughput, do nothing. In this scenario, some cores are not helping to increase the maximum possible throughput as their PMD threads have no useful work to do.&lt;/p&gt; &lt;p&gt;For example, in Figure 1, &lt;code&gt;dpdk0&lt;/code&gt; and &lt;code&gt;dpdk1&lt;/code&gt; ports have a lot of traffic and the PMD thread on core 8 is overloaded processing those packets. The &lt;code&gt;dpdk2&lt;/code&gt; and &lt;code&gt;dpdk3&lt;/code&gt; ports have no traffic and the PMD thread on core 10 is idle.&lt;/p&gt; &lt;div id="attachment_877517" style="width: 572px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/03/1-ovs-dpdk-overload-sketch.png"&gt;&lt;img aria-describedby="caption-attachment-877517" class="wp-image-877517 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/1-ovs-dpdk-overload-sketch.png" alt="The dpdk2 and dpdk3 threads have no traffic and the PMD thread on core 10 is idle." width="562" height="242" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/03/1-ovs-dpdk-overload-sketch.png 562w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/1-ovs-dpdk-overload-sketch-300x129.png 300w" sizes="(max-width: 562px) 100vw, 562px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-877517" class="wp-caption-text"&gt;Figure 1: The packet processing load is not balanced across PMD threads.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Whenever there is a reconfiguration, such as adding new ports in OVS-DPDK, Rx queues are reassigned to PMD threads. The primary goal of reassigning is that the Rx queues requiring the most processing are assigned to different PMD threads, so as many PMD threads as possible get to do useful work.&lt;/p&gt; &lt;p&gt;But what happens if the load was not known during the last reconfiguration? Reasons might be that a port was just added, or traffic wasn’t started yet, or the load changed over time and there are no more reconfigurations. This is where the PMD auto load balance feature can help.&lt;/p&gt; &lt;h2&gt;PMD auto load balancing&lt;/h2&gt; &lt;p&gt;If the PMD auto load balance feature is enabled, it runs periodically. When a certain set of conditions are met, it triggers a reassignment of the Rx queues to PMD threads to improve the balance of load between the PMD threads.&lt;/p&gt; &lt;p&gt;These are the main conditions it checks as prerequisites:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;If any of the current PMD threads are very busy processing packets.&lt;/li&gt; &lt;li&gt;If the variance between the PMD thread loads is likely to improve after a reassignment.&lt;/li&gt; &lt;li&gt;If it is not too soon since the last reassignment.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We can set exactly what constitutes &lt;em&gt;very busy&lt;/em&gt;, &lt;em&gt;improvement&lt;/em&gt;, and &lt;em&gt;too soon&lt;/em&gt; from the command line. We’ll see that when we look at the user parameters shortly.&lt;/p&gt; &lt;p&gt;The nice thing about this feature is that it will only do a reassignment if it detects that a PMD thread is currently very busy &lt;em&gt;and&lt;/em&gt; it estimates that there will be an improvement in variance after the reassignment. This helps to ensure we don’t have unnecessary reassignments.&lt;/p&gt; &lt;h2&gt;User parameters for PMD auto load balancing&lt;/h2&gt; &lt;p&gt;The PMD auto load balance feature is disabled by default. You can enable it at any time with:&lt;/p&gt; &lt;pre&gt;$ ovs-vsctl --no-wait set open_vSwitch . other_config:pmd-auto-lb="true"&lt;/pre&gt; &lt;p&gt;Let&amp;#8217;s look at the user parameters for this feature.&lt;/p&gt; &lt;h3&gt;Load threshold&lt;/h3&gt; &lt;p&gt;The &lt;em&gt;load threshold&lt;/em&gt; is the percentage of processing cycles one of the PMD threads must consistently be using for one minute before a reassignment can occur. The default is 95%. Since Open vSwitch 2.15, you can set this from the command line. To set it to 70%, you would enter the following:&lt;/p&gt; &lt;pre&gt;$ ovs-vsctl --no-wait set open_vSwitch . other_config:pmd-auto-lb-load-threshold="70"&lt;/pre&gt; &lt;h3&gt;Improvement threshold&lt;/h3&gt; &lt;p&gt;The &lt;em&gt;improvement threshold&lt;/em&gt; is the estimated improvement in load variance between the PMD threads that must be met before a reassignment can occur. To calculate the estimated improvement, a dry run of the reassignment is done and the estimated load variance is compared with the current variance. The default is 25%. Since Open vSwitch 2.15, you can set this from the command line. To set it to 50%, you would enter the following:&lt;/p&gt; &lt;pre&gt;$ ovs-vsctl --no-wait set open_vSwitch . other_config:pmd-auto-lb-improvement-threshold="50"&lt;/pre&gt; &lt;h3&gt;Interval threshold&lt;/h3&gt; &lt;p&gt;The &lt;em&gt;interval threshold&lt;/em&gt; is the minimum time in minutes between which two reassignments can be triggered. This is used to prevent triggering frequent reassignments where traffic patterns are changeable. For example, you might only want to trigger a reassignment once every 10 minutes or every few hours. The default is one minute; to set it to 10 minutes, you would enter:&lt;/p&gt; &lt;pre&gt;$ ovs-vsctl --no-wait set open_vSwitch . other_config:pmd-auto-lb-rebal-interval="10"&lt;/pre&gt; &lt;h2&gt;PMD auto load balancing in action&lt;/h2&gt; &lt;p&gt;Let’s revisit the simple example from Figure 1 to see the PMD auto load balance feature in action. As shown in Figure 2, we start with all traffic on one PMD thread on core 8 and no traffic on the PMD thread on core 10.&lt;/p&gt; &lt;div id="attachment_877517" style="width: 572px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/03/1-ovs-dpdk-overload-sketch.png"&gt;&lt;img aria-describedby="caption-attachment-877517" class="wp-image-877517 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/1-ovs-dpdk-overload-sketch.png" alt="The diagram shows that core 8 is overloaded, while core 10 has no traffic." width="562" height="242" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/03/1-ovs-dpdk-overload-sketch.png 562w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/1-ovs-dpdk-overload-sketch-300x129.png 300w" sizes="(max-width: 562px) 100vw, 562px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-877517" class="wp-caption-text"&gt;Figure 2: One PMD thread on core 8 has all the traffic.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;We can check the Rx queue assignments with the following:&lt;/p&gt; &lt;pre&gt;$ ovs-appctl dpif-netdev/pmd-rxq-show&lt;/pre&gt; &lt;p&gt;The following output confirms that the PMD thread on core 8 has two active Rx queues while the PMD thread on core 10 has none:&lt;/p&gt; &lt;pre&gt;pmd thread numa_id 0 core_id 8: isolated : false port: dpdk0 queue-id: 0 (enabled) pmd usage: 47 % port: dpdk1 queue-id: 0 (enabled) pmd usage: 47 % pmd thread numa_id 0 core_id 10: isolated : false port: dpdk2 queue-id: 0 (enabled) pmd usage: 0 % port: dpdk3 queue-id: 0 (enabled) pmd usage: 0 % &lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: The percentage usage shown for each Rx queue here is tightly measured around processing packets for individual Rx queues. It does not include any PMD thread operational overhead or time spent polling while getting no packets. Use &lt;code&gt;ovs-appctl dpif-netdev/pmd-stats-show&lt;/code&gt; to get more general PMD thread load totals.&lt;/p&gt; &lt;h3&gt;A case for reassignment&lt;/h3&gt; &lt;p&gt;At this point, the traffic generator indicates a maximum of 11 Mpps bi-directional throughput. This is certainly a case where the PMD auto load balance feature can help by triggering a reassignment. Following the reassignment, the PMD thread on core 10 should be utilized.&lt;/p&gt; &lt;p&gt;Let’s set some thresholds and enable the feature:&lt;/p&gt; &lt;pre&gt;$ ovs-vsctl set open_vSwitch . other_config:pmd-auto-lb-load-threshold="80" $ ovs-vsctl set open_vSwitch . other_config:pmd-auto-lb-improvement-threshold="50" $ ovs-vsctl set open_vSwitch . other_config:pmd-auto-lb-rebal-interval="1" $ ovs-vsctl set open_vSwitch . other_config:pmd-auto-lb="true" &lt;/pre&gt; &lt;p&gt;The logs confirm it has been enabled and the values we have set:&lt;/p&gt; &lt;pre&gt;|dpif_netdev|INFO|PMD auto load balance is enabled interval 1 mins, pmd load threshold 80%, improvement threshold 50%&lt;/pre&gt; &lt;p&gt;Soon, we see that a dry run has been completed and reconfiguration of the datapath has been requested to reassign the Rx queues to the PMD threads:&lt;/p&gt; &lt;pre&gt;|dpif_netdev|INFO|PMD auto lb dry run. requesting datapath reconfigure.&lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: You can get more detailed information about the operation and estimates in the &lt;code&gt;ovs-vswitchd.log&lt;/code&gt; file by enabling debug:&lt;/p&gt; &lt;pre style="padding-left: 40px;"&gt;$ ovs-appctl vlog/set dpif_netdev:file:dbg&lt;/pre&gt; &lt;h3&gt;Check the results&lt;/h3&gt; &lt;p&gt;Now, if we re-check the stats, we can confirm that the two Rx queues that are receiving packets are assigned to different PMD threads.&lt;/p&gt; &lt;pre&gt;pmd thread numa_id 0 core_id 8: isolated : false port: dpdk0 queue-id: 0 (enabled) pmd usage: 0 % port: dpdk2 queue-id: 0 (enabled) pmd usage: 88 % pmd thread numa_id 0 core_id 10: isolated : false port: dpdk1 queue-id: 0 (enabled) pmd usage: 88 % port: dpdk3 queue-id: 0 (enabled) pmd usage: 0 % &lt;/pre&gt; &lt;p&gt;We have gone from a situation where only one PMD thread was in use, to a case where both PMD threads are being utilized, as shown in Figure 3.&lt;/p&gt; &lt;div id="attachment_877567" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/03/4-ovs-dpdk-overunderload-horizontal-sketch.png"&gt;&lt;img aria-describedby="caption-attachment-877567" class="wp-image-877567 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/4-ovs-dpdk-overunderload-horizontal-sketch-1024x195.png" alt="The diagram shows a balanced traffic load." width="640" height="122" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/03/4-ovs-dpdk-overunderload-horizontal-sketch-1024x195.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/4-ovs-dpdk-overunderload-horizontal-sketch-300x57.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/4-ovs-dpdk-overunderload-horizontal-sketch-768x146.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/4-ovs-dpdk-overunderload-horizontal-sketch.png 1274w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-877567" class="wp-caption-text"&gt;Figure 3: Both PMD threads are fully utilized.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Checking the traffic generator, in this case, the throughput has risen from 11 Mpps with packet drops on both &lt;code&gt;dpdk0&lt;/code&gt; and &lt;code&gt;dpdk1&lt;/code&gt; ports to the maximum configured rate of 21 Mpps without any drops.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: In the example above, each port has one Rx queue, but in many cases, the workload can also be distributed with more granularity by using multiple Rx queues per port and RSS.&lt;/p&gt; &lt;h2&gt;Current limitations of PMD auto load balancing&lt;/h2&gt; &lt;p&gt;Of course, the example shown is a clear-cut case to illustrate the parameters and usage.&lt;/p&gt; &lt;p&gt;The reassignment code will assign the largest loaded Rx queues to different PMD threads. It will also try to ensure that all PMD threads have the same number of assigned Rx queues. This is to find a compromise between optimizing for the current traffic load and providing resilience for cases where traffic patterns might change and there is no PMD auto load balance.&lt;/p&gt; &lt;p&gt;In some cases, it is possible that assigning different numbers of Rx queues to PMD threads could give an even more improved balance for the current load. This is currently only available by &lt;a target="_blank" rel="nofollow" href="https://docs.openvswitch.org/en/latest/topics/dpdk/pmd/#port-rx-queue-assigment-to-pmd-threads"&gt;manual pinning&lt;/a&gt; and is an area for potential future optimizations.&lt;/p&gt; &lt;h2&gt;Wrap-up&lt;/h2&gt; &lt;p&gt;In this article, we’ve looked at the Open vSwitch with DPDK PMD auto load balance feature, the problem it helps to solve, and how it operates. The feature is taking shape and we&amp;#8217;ve added new user parameters in Open vSwitch 2.15. Feel free to try it out and give feedback on the ovs-discuss@openvswitch.org mailing list.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F29%2Fautomatic-load-balancing-for-pmd-threads-in-open-vswitch-with-dpdk%2F&amp;#38;linkname=Automatic%20load%20balancing%20for%20PMD%20threads%20in%20Open%20vSwitch%20with%20DPDK" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F29%2Fautomatic-load-balancing-for-pmd-threads-in-open-vswitch-with-dpdk%2F&amp;#38;linkname=Automatic%20load%20balancing%20for%20PMD%20threads%20in%20Open%20vSwitch%20with%20DPDK" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F29%2Fautomatic-load-balancing-for-pmd-threads-in-open-vswitch-with-dpdk%2F&amp;#38;linkname=Automatic%20load%20balancing%20for%20PMD%20threads%20in%20Open%20vSwitch%20with%20DPDK" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F29%2Fautomatic-load-balancing-for-pmd-threads-in-open-vswitch-with-dpdk%2F&amp;#38;linkname=Automatic%20load%20balancing%20for%20PMD%20threads%20in%20Open%20vSwitch%20with%20DPDK" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F29%2Fautomatic-load-balancing-for-pmd-threads-in-open-vswitch-with-dpdk%2F&amp;#38;linkname=Automatic%20load%20balancing%20for%20PMD%20threads%20in%20Open%20vSwitch%20with%20DPDK" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F29%2Fautomatic-load-balancing-for-pmd-threads-in-open-vswitch-with-dpdk%2F&amp;#38;linkname=Automatic%20load%20balancing%20for%20PMD%20threads%20in%20Open%20vSwitch%20with%20DPDK" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F29%2Fautomatic-load-balancing-for-pmd-threads-in-open-vswitch-with-dpdk%2F&amp;#38;linkname=Automatic%20load%20balancing%20for%20PMD%20threads%20in%20Open%20vSwitch%20with%20DPDK" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F29%2Fautomatic-load-balancing-for-pmd-threads-in-open-vswitch-with-dpdk%2F&amp;#038;title=Automatic%20load%20balancing%20for%20PMD%20threads%20in%20Open%20vSwitch%20with%20DPDK" data-a2a-url="https://developers.redhat.com/blog/2021/04/29/automatic-load-balancing-for-pmd-threads-in-open-vswitch-with-dpdk/" data-a2a-title="Automatic load balancing for PMD threads in Open vSwitch with DPDK"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/29/automatic-load-balancing-for-pmd-threads-in-open-vswitch-with-dpdk/"&gt;Automatic load balancing for PMD threads in Open vSwitch with DPDK&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/InvJnZZDzLo" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;This article is about the poll mode driver (PMD) automatic load balance feature in Open vSwitch with a Data Plane Development Kit data path (OVS-DPDK). The feature has existed for a while but we&amp;#8217;ve recently added new user parameters in Open vSwitch 2.15. Now is a good time to take a look at this feature [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/29/automatic-load-balancing-for-pmd-threads-in-open-vswitch-with-dpdk/"&gt;Automatic load balancing for PMD threads in Open vSwitch with DPDK&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/04/29/automatic-load-balancing-for-pmd-threads-in-open-vswitch-with-dpdk/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">871477</post-id><dc:creator>Kevin Traynor</dc:creator><dc:date>2021-04-29T07:00:51Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/04/29/automatic-load-balancing-for-pmd-threads-in-open-vswitch-with-dpdk/</feedburner:origLink></entry><entry><title>Containerize .NET for Red Hat OpenShift: Use a Windows VM like a container</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/T-IDo8msTQ8/" /><category term=".NET" /><category term="Containers" /><category term="Kubernetes" /><category term="Windows" /><category term="OpenShift Virtualization" /><category term="Windows containers" /><category term="Windows VM containers" /><author><name>Don Schenck</name></author><id>https://developers.redhat.com/blog/?p=885137</id><updated>2021-04-29T07:00:43Z</updated><published>2021-04-29T07:00:43Z</published><content type="html">&lt;p&gt;Embracing the future—making the transition from legacy monolithic applications running on .NET Framework to &lt;a target="_blank" rel="nofollow" href="/topics/microservices"&gt;microservices&lt;/a&gt; and images running in &lt;a target="_blank" rel="nofollow" href="/topics/containers"&gt;containers&lt;/a&gt; (or pods)—is a tall task. If only there were a safe, proceed-at-your-own-pace way to make the change, one that was familiar yet led to a new destination. Of course, there is such a path; otherwise, I wouldn&amp;#8217;t be writing this article. In this article, the last in my series introducing &lt;a target="_blank" rel="nofollow" href="/blog/2021/03/16/three-ways-to-containerize-net-applications-on-red-hat-openshift/"&gt;three ways to containerize .NET applications on Red Hat OpenShift&lt;/a&gt;, we&amp;#8217;ll look at running Windows virtual machines (VMs) on &lt;a target="_blank" rel="nofollow" href="/products/openshift/overview"&gt;OpenShift&lt;/a&gt;, and treating them like containers.&lt;/p&gt; &lt;p&gt;In case you missed them, here are the two other articles in the &amp;#8220;Containerize .NET for Red Hat OpenShift&amp;#8221; series:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="/blog/2021/04/22/containerize-net-for-red-hat-openshift-windows-containers-and-net-framework/"&gt;Windows containers and .NET Framework&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="/blog/2021/04/15/containerize-net-for-red-hat-openshift-linux-containers-and-net-core/"&gt;Linux containers and .NET Core&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;span id="more-885137"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;Reframing &amp;#8216;legacy&amp;#8217; applications&lt;/h2&gt; &lt;p&gt;Apropos of nothing, I read an idea that I like, and it might help minimize the bad connotation that often comes with the phrase &amp;#8220;legacy application.&amp;#8221; All you have to do is replace the word &lt;em&gt;legacy&lt;/em&gt; with &lt;em&gt;proven&lt;/em&gt;. Nice, eh? Suddenly, you&amp;#8217;re not working on some old code that is barely doing the job. Instead, it&amp;#8217;s a proven system that is being upgraded with newer technology.&lt;/p&gt; &lt;p&gt;Okay, back to the topic at hand.&lt;/p&gt; &lt;h2&gt;Running Windows VMs on OpenShift&lt;/h2&gt; &lt;p&gt;Our goal is to run an existing &lt;a target="_blank" rel="nofollow" href="/blog/category/windows/"&gt;Windows&lt;/a&gt; virtual machine on OpenShift but treat it— from an operations perspective—as a container. We also want to keep the &amp;#8220;VM-ness&amp;#8221; that we&amp;#8217;re accustomed to, so we&amp;#8217;ll use the &lt;a target="_blank" rel="nofollow" href="https://www.openshift.com/learn/topics/virtualization/"&gt;OpenShift Virtualization Operator&lt;/a&gt;, which is built on &lt;a target="_blank" rel="nofollow" href="https://kubevirt.io/"&gt;KubeVirt technology&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;OpenShift Virtualization allows you to run a virtual machine (hint: it&amp;#8217;s not limited to Windows VMs) and have it appear within your cluster just how an image would. You get the same role-based access control, service discovery, and other features that you would with any image. Your web service running in IIS appears as &lt;em&gt;just another service&lt;/em&gt; in OpenShift. Your other applications can access it by service name; no ports, no IP addresses, no server names &amp;#8230; just the name.&lt;/p&gt; &lt;p&gt;And it works both ways: Your IIS website can access your other services by name. This could even include an SQL Server database.&lt;/p&gt; &lt;p&gt;Now I have your attention. Let&amp;#8217;s start setting up our development environment.&lt;/p&gt; &lt;h2&gt;Install the OpenShift Virtualization Operator&lt;/h2&gt; &lt;p&gt;The first step is to install the OpenShift Virtualization Operator. This is incredibly simple:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Search for and locate the OpenShift Virtualization Operator.&lt;/li&gt; &lt;li&gt;Click &lt;b&gt;Install&lt;/b&gt;.&lt;/li&gt; &lt;li&gt;Click &lt;b&gt;Install&lt;/b&gt; again.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Just like that, you&amp;#8217;ve installed the Operator. All that remains is to create a HyperConverged cluster:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Click &lt;b&gt;OpenShift Virtualization Deployment&lt;/b&gt;.&lt;/li&gt; &lt;li&gt;Click &lt;b&gt;Create HyperConverged&lt;/b&gt;.&lt;/li&gt; &lt;li&gt;Click &lt;b&gt;Create&lt;/b&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Note&lt;/b&gt;: &lt;a target="_blank" rel="nofollow" href="/blog/2020/08/28/enable-openshift-virtualization-on-red-hat-openshift/"&gt;This article&lt;/a&gt; has detailed instructions for installing the OpenShift Virtualization Operator.&lt;/p&gt; &lt;h2&gt;Install the virtctl CLI&lt;/h2&gt; &lt;p&gt;Life is easier if you use the command-line interface (CLI) tool, &lt;code&gt;virtctl&lt;/code&gt;. You can install it by following the &lt;a target="_blank" rel="nofollow" href="https://kubevirt.io/user-guide/operations/virtctl_client_tool/"&gt;instructions on this web page&lt;/a&gt;. The &lt;code&gt;virtctl&lt;/code&gt; CLI is handy for uploading virtual machine images into your cluster.&lt;/p&gt; &lt;h2&gt;Prepare the Hyper-V VM&lt;/h2&gt; &lt;p&gt;We&amp;#8217;ll use the &lt;a target="_blank" rel="nofollow" href="https://cloudbase.it/qemu-img-windows/"&gt;qemu-img command-line tool&lt;/a&gt; to convert the Hyper-V VM into QEMU copy-on-write (QCOW2) format. This is one of the formats that CNV is expecting. Here&amp;#8217;s the command I used:&lt;/p&gt; &lt;pre&gt;qemu-img convert "C:\Users\Public\Documents\Hyper-V\Virtual hard disks\win2012r2.vhdx" win2012r2.qcow2 -O qcow2 &lt;/pre&gt; &lt;p&gt;The conversion finished rather quickly on my machine. So quick, in fact, that I double-checked to make sure the output file was there. It&amp;#8217;s shown in Figure 1.&lt;/p&gt; &lt;div id="attachment_889397" style="width: 610px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/03/ls-output.png"&gt;&lt;img aria-describedby="caption-attachment-889397" class="wp-image-889397 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/ls-output.png" alt="Output of the LS command confirms the installation." width="600" height="139" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/03/ls-output.png 600w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/ls-output-300x70.png 300w" sizes="(max-width: 600px) 100vw, 600px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-889397" class="wp-caption-text"&gt;Figure 1: The output file confirms the installation.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Once you have this 21GB file at hand, the fun begins.&lt;/p&gt; &lt;h2&gt;Upload the Hyper-V VM image to your cluster&lt;/h2&gt; &lt;p&gt;Uploading the Hyper-V VM image to a cluster turned out to be more of a project than I anticipated, but I came up with a workaround that I&amp;#8217;ll share with you here.&lt;/p&gt; &lt;p&gt;I&amp;#8217;ve uploaded and run scores of virtual machines. I typically use a smaller VM image, about 4GB, and it takes 45 minutes using my slow home internet. If something goes wrong, I wipe things out and start over; another hour or so lost.&lt;/p&gt; &lt;p&gt;Based on this experience, uploading a 21GB file would take hours, and I couldn&amp;#8217;t afford the time to upload it again if something went wrong. Knowing it was possible to pull a VM image into my cluster from a URL gave me a better idea.&lt;/p&gt; &lt;h2&gt;Amazon S3 to the rescue&lt;/h2&gt; &lt;p&gt;I decided to create a Simple Storage Service (S3) bucket in my Amazon Web Services account and upload my 21GB image to that. This would let me pull from the S3 URL straight into my cluster. AWS and Azure—where I&amp;#8217;m using &lt;a target="_blank" rel="nofollow" href="https://azure.microsoft.com/en-us/services/openshift/"&gt;Azure Red Hat OpenShift&lt;/a&gt; to host my cluster—both have super high-speed internet connections. Creating a VM should be quick.&lt;/p&gt; &lt;p&gt;It took four hours for my image to upload to the S3 bucket; I launched it in the evening and went to bed, fingers crossed that it would succeed, and it did.&lt;/p&gt; &lt;p&gt;In the end, it now takes about &lt;em&gt;four minutes&lt;/em&gt; to pull the image into my cluster. Software development isn&amp;#8217;t &lt;em&gt;always&lt;/em&gt; about failures; we have our winning moments, too. This was one of them, and I enjoyed it.&lt;/p&gt; &lt;h2&gt;Create the Windows VM in OpenShift&lt;/h2&gt; &lt;p&gt;Once the OpenShift container-native virtualization (CNV) &lt;em&gt;mise en place&lt;/em&gt; is ready, it&amp;#8217;s time to make things happen. Follow along with me here.&lt;/p&gt; &lt;p&gt;First, I logged in at the command-line and created my project, &lt;code&gt;winquotes&lt;/code&gt;, as shown in Figure 2.&lt;/p&gt; &lt;div id="attachment_889487" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/03/oc-new-project-winquotes.png"&gt;&lt;img aria-describedby="caption-attachment-889487" class="wp-image-889487" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/oc-new-project-winquotes.png" alt="Use the oc new-project command to create a new project." width="640" height="38" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/03/oc-new-project-winquotes.png 782w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/oc-new-project-winquotes-300x18.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/oc-new-project-winquotes-768x46.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-889487" class="wp-caption-text"&gt;Figure 2: Create the project on the command line.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;The remaining work happens in the Red Hat OpenShift administrator dashboard. To start, within the &lt;b&gt;winquotes&lt;/b&gt; project, I selected the &lt;b&gt;Virtualization&lt;/b&gt; option (shown in Figure 3) to start creating a new virtual machine.&lt;/p&gt; &lt;div id="attachment_889507" style="width: 563px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/03/winquotes-virtualization.png"&gt;&lt;img aria-describedby="caption-attachment-889507" class="wp-image-889507 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/winquotes-virtualization.png" alt="The virtualization option in the OpenShift project dashboard." width="553" height="437" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/03/winquotes-virtualization.png 553w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/winquotes-virtualization-300x237.png 300w" sizes="(max-width: 553px) 100vw, 553px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-889507" class="wp-caption-text"&gt;Figure 3: Open the project and select the virtualization option.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;The &lt;b&gt;New with Wizard&lt;/b&gt; option stepped me through the process, as shown in Figure 4.&lt;/p&gt; &lt;div id="attachment_889527" style="width: 426px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/03/virtualization-new-with-wizard.png"&gt;&lt;img aria-describedby="caption-attachment-889527" class="wp-image-889527 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2021/03/virtualization-new-with-wizard.png" alt="The virtualization 'New with wizard option' in OpenShift." width="416" height="338" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/03/virtualization-new-with-wizard.png 416w, https://developers.redhat.com/blog/wp-content/uploads/2021/03/virtualization-new-with-wizard-300x244.png 300w" sizes="(max-width: 416px) 100vw, 416px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-889527" class="wp-caption-text"&gt;Figure 4: Select the &amp;#8216;New with wizard&amp;#8217; option.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;As I mentioned in the last section, the VM instance is imported from the S3 URL. For demonstration purposes, I kept the CPU and RAM constraints low: 2GB of RAM and only two CPUs, as shown in Figure 5.&lt;/p&gt; &lt;div id="attachment_892467" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_general.png"&gt;&lt;img aria-describedby="caption-attachment-892467" class="wp-image-892467" src="https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_general.png" alt="The opening dialog for creating a new virtual machine." width="640" height="859" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_general.png 668w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_general-223x300.png 223w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-892467" class="wp-caption-text"&gt;Figure 5: Create a new virtual machine.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;It&amp;#8217;s important to edit the disk information, especially because the default value for the root disk is 15GB. The disk must be large enough to hold the original VM, which in my case had a 50GB drive. I edited the size to 60GB to be safe. I also changed the drive type to SATA because it&amp;#8217;s Windows. Figure 6 shows the default storage settings.&lt;/p&gt; &lt;div id="attachment_892477" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_storage_1.png"&gt;&lt;img aria-describedby="caption-attachment-892477" class="wp-image-892477" src="https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_storage_1.png" alt="Disk storage with default values." width="640" height="164" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_storage_1.png 818w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_storage_1-300x77.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_storage_1-768x197.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-892477" class="wp-caption-text"&gt;Figure 6: The default storage settings.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Figure 7 shows my edited storage settings.&lt;/p&gt; &lt;div id="attachment_892487" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_storage_2.png"&gt;&lt;img aria-describedby="caption-attachment-892487" class="wp-image-892487" src="https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_storage_2.png" alt="Disk storage values edited to support a Windows VM." width="640" height="241" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_storage_2.png 817w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_storage_2-300x113.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_storage_2-768x290.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-892487" class="wp-caption-text"&gt;Figure 7: The edited storage settings.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;I was able to go with the default values on the screen for the remainder of the process to create this virtual machine. When I reached the final screen, it immediately began importing the VM located at the supplied S3 URL, as shown in Figure 8.&lt;/p&gt; &lt;div id="attachment_892527" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_importing.png"&gt;&lt;img aria-describedby="caption-attachment-892527" class="wp-image-892527" src="https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_importing.png" alt="The VM import is automatic." width="640" height="262" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_importing.png 807w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_importing-300x123.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_importing-768x314.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-892527" class="wp-caption-text"&gt;Figure 8: Importing the virtual machine.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;When the import was finished, the VM was turned off, as shown in Figure 9.&lt;/p&gt; &lt;div id="attachment_893517" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_status_off.png"&gt;&lt;img aria-describedby="caption-attachment-893517" class="wp-image-893517" src="https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_status_off.png" alt="The VM status is off." width="640" height="233" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_status_off.png 770w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_status_off-300x109.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_status_off-768x279.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-893517" class="wp-caption-text"&gt;Figure 9: The virtual machine status is off.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;I started it from the menu in the upper-right corner, as shown in Figure 10.&lt;/p&gt; &lt;div id="attachment_892547" style="width: 200px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_start_menu_option.png"&gt;&lt;img aria-describedby="caption-attachment-892547" class="wp-image-892547 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_start_menu_option.png" alt="Select the option to start the virtual machine." width="190" height="288" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-892547" class="wp-caption-text"&gt;Figure 10: Start the virtual machine manually.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;I then switched to OpenShift&amp;#8217;s console view and waited for my login screen. As shown in Figure 11, my Windows VM was up and running in OpenShift.&lt;/p&gt; &lt;div id="attachment_893507" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_log_on-1.png"&gt;&lt;img aria-describedby="caption-attachment-893507" class="wp-image-893507 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_log_on-1-1024x562.png" alt="The Windows VM login screen." width="640" height="351" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_log_on-1-1024x562.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_log_on-1-300x165.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_log_on-1-768x422.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_log_on-1.png 1433w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-893507" class="wp-caption-text"&gt;Figure 11: The virtual machine has started.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;I then went to open the IIS application in my browser and &lt;code&gt;localhost:8081/api/quotes&lt;/code&gt;, expecting to see it working. But it wasn&amp;#8217;t working. The application could not connect to the database running in Azure.&lt;/p&gt; &lt;p&gt;As it turns out, the VM cannot connect to the network until the VirtIO network driver is installed.&lt;/p&gt; &lt;h2&gt;How to install the VirtIO network driver&lt;/h2&gt; &lt;p&gt;If you&amp;#8217;ve followed along with me so far, you can go into your CD drive and install the guest tools, including the VirtIO driver. After that, your application will be connected to the internet. Figure 12 shows the Windows file explorer with the CD ROM listed.&lt;/p&gt; &lt;div id="attachment_893497" style="width: 567px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_drives.png"&gt;&lt;img aria-describedby="caption-attachment-893497" class="wp-image-893497 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_drives.png" alt="Windows file explorer showing the CD drive." width="557" height="306" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_drives.png 557w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_drives-300x165.png 300w" sizes="(max-width: 557px) 100vw, 557px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-893497" class="wp-caption-text"&gt;Figure 12: Windows file explorer showing the VirtIO driver.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Now, your service should be able to reach your database, and it should be up and running. The next step is to convert the service into an OpenShift service and make it available to other applications within your cluster.&lt;/p&gt; &lt;h2&gt;Create and expose a service within a cluster&lt;/h2&gt; &lt;p&gt;We want to expose the application as a service within a cluster, which starts with creating a service. The command-line tool, &lt;code&gt;virtctl&lt;/code&gt;, is useful for this purpose. For my example, I specified the name of the VM, the port I wanted to expose—8081 in my case—and assigned a name to the service. I used the following command to do all that:&lt;/p&gt; &lt;pre&gt;virtctl expose vm quotesvm --port=8081 --name=quotes --type=NodePort &lt;/pre&gt; &lt;p&gt;Now, we have a service that is available by name (&amp;#8220;quotes,&amp;#8221; in this case) within the cluster. A service. Running in an OpenShift cluster. Running on IIS. Inside a Windows VM.&lt;/p&gt; &lt;p&gt;Mind, blown.&lt;/p&gt; &lt;p&gt;Finally, just like any other service within a cluster, we can use OpenShift&amp;#8217;s &lt;code&gt;oc&lt;/code&gt; command to expose this service to the world, as shown in Figure 13.&lt;/p&gt; &lt;div id="attachment_892677" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_expose_service.png"&gt;&lt;img aria-describedby="caption-attachment-892677" class="wp-image-892677" src="https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_expose_service.png" alt="Enter 'oc expose service quotes' to expose the service." width="640" height="64" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_expose_service.png 966w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_expose_service-300x30.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_expose_service-768x77.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-892677" class="wp-caption-text"&gt;Figure 13: Expose the service on the command line.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Now, we should be able to open a browser in any internet-connected machine and see the service results at the URL listed. Except, it doesn&amp;#8217;t work.&lt;/p&gt; &lt;p&gt;Why not?&lt;/p&gt; &lt;h2&gt;A VM is a VM is a VM&lt;/h2&gt; &lt;p&gt;Let&amp;#8217;s go back to the VM. Even though everything inside our cluster is aligned and configured correctly, the VM still needs a new Windows firewall rule to access port 8081. Don&amp;#8217;t forget: It&amp;#8217;s still a virtual machine and acts like one. You can manage and treat this VM like an OpenShift application in the context of OpenShift, but the underlying technology is still a VM. Adjust accordingly.&lt;/p&gt; &lt;p&gt;Once we establish the new firewall rule, we have success. Our service is available to the world. Figure 14 shows the service on my phone (a Surface Duo, by the way).&lt;/p&gt; &lt;div id="attachment_892717" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_running_on_phone.png"&gt;&lt;img aria-describedby="caption-attachment-892717" class="wp-image-892717" src="https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_running_on_phone.png" alt="A mobile phone web browser showing the running service." width="640" height="890" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_running_on_phone.png 727w, https://developers.redhat.com/blog/wp-content/uploads/2021/04/create_vm_running_on_phone-216x300.png 216w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-892717" class="wp-caption-text"&gt;Figure 14: The live service on a mobile phone browser.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Conclusion: What are you waiting for?&lt;/h2&gt; &lt;p&gt;You can run Windows VMs in Red Hat OpenShift and treat them like containers. You can &lt;a target="_blank" rel="nofollow" href="/blog/2021/04/22/containerize-net-for-red-hat-openshift-windows-containers-and-net-framework/"&gt;run Windows applications in Windows containers on OpenShift&lt;/a&gt;. You can &lt;a target="_blank" rel="nofollow" href="/blog/2021/04/15/containerize-net-for-red-hat-openshift-linux-containers-and-net-core/"&gt;create Linux containers using .NET Core&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;There are no blockers. You have the tools. Embrace the future.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F29%2Fcontainerize-net-for-red-hat-openshift-use-a-windows-vm-like-a-container%2F&amp;#38;linkname=Containerize%20.NET%20for%20Red%20Hat%20OpenShift%3A%20Use%20a%20Windows%20VM%20like%20a%20container" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F29%2Fcontainerize-net-for-red-hat-openshift-use-a-windows-vm-like-a-container%2F&amp;#38;linkname=Containerize%20.NET%20for%20Red%20Hat%20OpenShift%3A%20Use%20a%20Windows%20VM%20like%20a%20container" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F29%2Fcontainerize-net-for-red-hat-openshift-use-a-windows-vm-like-a-container%2F&amp;#38;linkname=Containerize%20.NET%20for%20Red%20Hat%20OpenShift%3A%20Use%20a%20Windows%20VM%20like%20a%20container" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F29%2Fcontainerize-net-for-red-hat-openshift-use-a-windows-vm-like-a-container%2F&amp;#38;linkname=Containerize%20.NET%20for%20Red%20Hat%20OpenShift%3A%20Use%20a%20Windows%20VM%20like%20a%20container" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F29%2Fcontainerize-net-for-red-hat-openshift-use-a-windows-vm-like-a-container%2F&amp;#38;linkname=Containerize%20.NET%20for%20Red%20Hat%20OpenShift%3A%20Use%20a%20Windows%20VM%20like%20a%20container" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F29%2Fcontainerize-net-for-red-hat-openshift-use-a-windows-vm-like-a-container%2F&amp;#38;linkname=Containerize%20.NET%20for%20Red%20Hat%20OpenShift%3A%20Use%20a%20Windows%20VM%20like%20a%20container" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F29%2Fcontainerize-net-for-red-hat-openshift-use-a-windows-vm-like-a-container%2F&amp;#38;linkname=Containerize%20.NET%20for%20Red%20Hat%20OpenShift%3A%20Use%20a%20Windows%20VM%20like%20a%20container" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F04%2F29%2Fcontainerize-net-for-red-hat-openshift-use-a-windows-vm-like-a-container%2F&amp;#038;title=Containerize%20.NET%20for%20Red%20Hat%20OpenShift%3A%20Use%20a%20Windows%20VM%20like%20a%20container" data-a2a-url="https://developers.redhat.com/blog/2021/04/29/containerize-net-for-red-hat-openshift-use-a-windows-vm-like-a-container/" data-a2a-title="Containerize .NET for Red Hat OpenShift: Use a Windows VM like a container"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/29/containerize-net-for-red-hat-openshift-use-a-windows-vm-like-a-container/"&gt;Containerize .NET for Red Hat OpenShift: Use a Windows VM like a container&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/T-IDo8msTQ8" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Embracing the future—making the transition from legacy monolithic applications running on .NET Framework to microservices and images running in containers (or pods)—is a tall task. If only there were a safe, proceed-at-your-own-pace way to make the change, one that was familiar yet led to a new destination. Of course, there is such a path; otherwise, [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/04/29/containerize-net-for-red-hat-openshift-use-a-windows-vm-like-a-container/"&gt;Containerize .NET for Red Hat OpenShift: Use a Windows VM like a container&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/04/29/containerize-net-for-red-hat-openshift-use-a-windows-vm-like-a-container/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">885137</post-id><dc:creator>Don Schenck</dc:creator><dc:date>2021-04-29T07:00:43Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/04/29/containerize-net-for-red-hat-openshift-use-a-windows-vm-like-a-container/</feedburner:origLink></entry></feed>
